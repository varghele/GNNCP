{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eed764e",
   "metadata": {},
   "source": [
    "# Python Notebook to train the GNN on COVINO data\n",
    "Here, a message passing GNN(graph neural net) is trained on graphs representing a frame of an MD simulation. Each frame has a target value that represents the probability of a conformation target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9a16770",
   "metadata": {},
   "outputs": [],
   "source": [
    "##IMPORTS\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data, Batch, DataLoader\n",
    "\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, LeakyReLU, BatchNorm1d, Sigmoid\n",
    "from torch_scatter import scatter_mean, scatter_add\n",
    "from torch_geometric.nn import MetaLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f862d2a7",
   "metadata": {},
   "source": [
    "# Load graph data into list and scale graph data\n",
    "Data is from \"2_raw_graph\", produced by \"1_MAKE_GRAPH_FROM_H5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "addac451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up a definition to scale graph data - TODO!!\n",
    "def scale_graph_data(graph):\n",
    "    graph.x = (graph.x-torch.mean(graph.x, dim=0))/torch.std(graph.x)\n",
    "    graph.edge_attr = (graph.edge_attr-torch.mean(graph.edge_attr, dim=0))/torch.std(graph.edge_attr)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9132e3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Time: 0.12s\n"
     ]
    }
   ],
   "source": [
    "#Set up list where graphs are stored\n",
    "list_of_graphs = []\n",
    "\n",
    "#Load graphs into list\n",
    "loc_file = \"2_raw_graphs/\"\n",
    "\n",
    "\n",
    "start_load = time.time()\n",
    "#Loading the graphs\n",
    "#for gr in os.listdir(loc_file)[:5]+os.listdir(loc_file)[-5:]: #Just take the first 100 frames as a quick tryout to save RAM\n",
    "for gr in os.listdir(loc_file)[::100]:\n",
    "    data = torch.load(loc_file+gr)\n",
    "    #data_sc = scale_graph_data(data)\n",
    "    list_of_graphs.append(data)\n",
    "print(\"Load Time: {}s\".format(np.around(time.time()-start_load,2)))\n",
    "\n",
    "TT_SPLIT = 0.8\n",
    "training_data = list_of_graphs[:int(len(list_of_graphs)*TT_SPLIT)]\n",
    "testing_data = list_of_graphs[int(len(list_of_graphs)*TT_SPLIT):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379043c0",
   "metadata": {},
   "source": [
    "# Set up Graph net blocks - edge, node, global - without Encoding!\n",
    "Edge, Node and Global models for the MetaLayer \\\n",
    "In general without BatchNorm (but possible), \\\n",
    "0/1 Hidden Layer, \\\n",
    "LeakyRelu activation \\\n",
    "Glorot uniform initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65ffd050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == torch.nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "class EdgeModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EdgeModel, self).__init__()\n",
    "        hidden = HIDDEN_EDGE\n",
    "        in_channels = EDGE_FEATURES+2*NODE_FEATURES\n",
    "        self.edge_mlp = Seq(Lin(in_channels, hidden), LeakyReLU(),# BatchNorm1d(hidden),\n",
    "                            Lin(hidden, EDGE_FEATURES)).apply(init_weights)\n",
    "\n",
    "    def forward(self, src, dest, edge_attr, u, batch):\n",
    "        # source, target: [E, F_x], where E is the number of edges.\n",
    "        # edge_attr: [E, F_e]\n",
    "        # u: [B, F_u], where B is the number of graphs.\n",
    "        # batch: [E] with max entry B - 1.\n",
    "        out = torch.cat([src, dest, edge_attr], 1)\n",
    "        return self.edge_mlp(out)\n",
    "\n",
    "class NodeModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NodeModel, self).__init__()\n",
    "        hidden=HIDDEN_NODE\n",
    "        in_channels_1 = EDGE_FEATURES+NODE_FEATURES\n",
    "        in_channels_2 = hidden+NODE_FEATURES\n",
    "        self.node_mlp_1 = Seq(Lin(in_channels_1, hidden), LeakyReLU(),# BatchNorm1d(hidden),\n",
    "                              Lin(hidden, hidden)).apply(init_weights)\n",
    "        self.node_mlp_2 = Seq(Lin(in_channels_2, hidden), LeakyReLU(),# BatchNorm1d(hidden),\n",
    "                              Lin(hidden, NODE_FEATURES)).apply(init_weights)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        # x: [N, F_x], where N is the number of nodes.\n",
    "        # edge_index: [2, E] with max entry N - 1.\n",
    "        # edge_attr: [E, F_e]\n",
    "        # u: [B, F_u]\n",
    "        # batch: [N] with max entry B - 1.\n",
    "        row, col = edge_index\n",
    "        out = torch.cat([x[row], edge_attr], dim=1)\n",
    "        out = self.node_mlp_1(out)\n",
    "        out = scatter_add(out, col, dim=0, dim_size=x.size(0))\n",
    "        out = torch.cat([x, out], dim=1)\n",
    "        return self.node_mlp_2(out)\n",
    "\n",
    "class GlobalModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalModel, self).__init__()\n",
    "        hidden = HIDDEN_GLOBAL\n",
    "        in_channels=NODE_FEATURES+EDGE_FEATURES\n",
    "\n",
    "        self.global_mlp = Seq(Lin(in_channels, hidden), LeakyReLU(),# BatchNorm1d(hidden),\n",
    "                              Lin(hidden, GLOBAL_FEATURES)).apply(init_weights)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        # x: [N, F_x], where N is the number of nodes.\n",
    "        # edge_index: [2, E] with max entry N - 1.\n",
    "        # edge_attr: [E, F_e]\n",
    "        # u: [B, F_u]\n",
    "        # batch: [N] with max entry B - 1.\n",
    "        row,col=edge_index\n",
    "        node_aggregate = scatter_add(x, batch, dim=0)\n",
    "        edge_aggregate = scatter_add(edge_attr, batch[col], dim=0)\n",
    "        out = torch.cat([node_aggregate, edge_aggregate], dim=1)\n",
    "        return self.global_mlp(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f396aac5",
   "metadata": {},
   "source": [
    "# Set up Graph net blocks - edge, node, global - WITH Encoding!\n",
    "Edge, Node and Global models for the MetaLayer \\\n",
    "In general without BatchNorm (but possible), \\\n",
    "0/1 Hidden Layer, \\\n",
    "LeakyRelu activation \\\n",
    "Glorot uniform initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb227d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == torch.nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "class EdgeModel_ENC(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EdgeModel_ENC, self).__init__()\n",
    "        hidden = HIDDEN_EDGE\n",
    "        in_channels = ENCODING_SIZE+2*ENCODING_SIZE\n",
    "        self.edge_mlp = Seq(Lin(in_channels, hidden), LeakyReLU(),# BatchNorm1d(hidden),\n",
    "                            Lin(hidden, ENCODING_SIZE)).apply(init_weights)\n",
    "\n",
    "    def forward(self, src, dest, edge_attr, u, batch):\n",
    "        # source, target: [E, F_x], where E is the number of edges.\n",
    "        # edge_attr: [E, F_e]\n",
    "        # u: [B, F_u], where B is the number of graphs.\n",
    "        # batch: [E] with max entry B - 1.\n",
    "        out = torch.cat([src, dest, edge_attr], 1)\n",
    "        return self.edge_mlp(out)\n",
    "\n",
    "class NodeModel_ENC(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NodeModel_ENC, self).__init__()\n",
    "        hidden=HIDDEN_NODE\n",
    "        in_channels_1 = ENCODING_SIZE+ENCODING_SIZE\n",
    "        in_channels_2 = hidden+ENCODING_SIZE\n",
    "        self.node_mlp_1 = Seq(Lin(in_channels_1, hidden), LeakyReLU(),# BatchNorm1d(hidden),\n",
    "                              Lin(hidden, hidden)).apply(init_weights)\n",
    "        self.node_mlp_2 = Seq(Lin(in_channels_2, hidden), LeakyReLU(),# BatchNorm1d(hidden),\n",
    "                              Lin(hidden, ENCODING_SIZE)).apply(init_weights)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        # x: [N, F_x], where N is the number of nodes.\n",
    "        # edge_index: [2, E] with max entry N - 1.\n",
    "        # edge_attr: [E, F_e]\n",
    "        # u: [B, F_u]\n",
    "        # batch: [N] with max entry B - 1.\n",
    "        row, col = edge_index\n",
    "        out = torch.cat([x[row], edge_attr], dim=1)\n",
    "        out = self.node_mlp_1(out)\n",
    "        out = scatter_add(out, col, dim=0, dim_size=x.size(0))\n",
    "        out = torch.cat([x, out], dim=1)\n",
    "        return self.node_mlp_2(out)\n",
    "\n",
    "class GlobalModel_ENC(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalModel_ENC, self).__init__()\n",
    "        hidden = HIDDEN_GLOBAL\n",
    "        in_channels=ENCODING_SIZE+ENCODING_SIZE\n",
    "\n",
    "        self.global_mlp = Seq(Lin(in_channels, hidden), LeakyReLU(),# BatchNorm1d(hidden),\n",
    "                              Lin(hidden, GLOBAL_FEATURES)).apply(init_weights)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        # x: [N, F_x], where N is the number of nodes.\n",
    "        # edge_index: [2, E] with max entry N - 1.\n",
    "        # edge_attr: [E, F_e]\n",
    "        # u: [B, F_u]\n",
    "        # batch: [N] with max entry B - 1.\n",
    "        row,col=edge_index\n",
    "        node_aggregate = scatter_add(x, batch, dim=0)\n",
    "        edge_aggregate = scatter_add(edge_attr, batch[col], dim=0)\n",
    "        out = torch.cat([node_aggregate, edge_aggregate], dim=1)\n",
    "        #out = torch.cat([node_aggregate], dim=1)\n",
    "        return self.global_mlp(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d188d8",
   "metadata": {},
   "source": [
    "# Set up GNN model - GNNCP\n",
    "This GNN model takes input graphs of each frame, and predicts the conformation probability as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d29a2f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNCP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GNNCP, self).__init__()\n",
    "        \n",
    "        #Define constants for network\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.global_feats = GLOBAL_FEATURES\n",
    "        self.no_mp = NO_MP\n",
    "        \n",
    "        self.mlp_hidden = MLP_HIDDEN\n",
    "        self.targ_size = TARG_SIZE\n",
    "        \n",
    "        #Define MP layer - MetaLayer Class\n",
    "        self.meta = MetaLayer(EdgeModel(), NodeModel(), GlobalModel())\n",
    "\n",
    "        #Define last stage MLP that makes prediction from global\n",
    "        self.mlp = Seq(Lin(self.global_feats, self.mlp_hidden), LeakyReLU(), #BatchNorm1d(self.mlp_hidden), \n",
    "                       Lin(self.mlp_hidden, self.mlp_hidden), LeakyReLU(), #BatchNorm1d(self.mlp_hidden), \n",
    "                       Lin(self.mlp_hidden, self.targ_size)).apply(init_weights)\n",
    "    \n",
    "    def forward(self, g):\n",
    "        #Extract tensors from input graph\n",
    "        x, ea, ei, btc = g.x, g.edge_attr, g.edge_index, g.batch\n",
    "        \n",
    "        #Set up global attribute vector for message passing\n",
    "        #careful: has to be dtype torch float 32, and be manually passed to device in case of GPU\n",
    "        u = torch.full(size=(self.batch_size, self.global_feats), fill_value = 1, dtype=torch.float).to(device)\n",
    "        \n",
    "        #Perform the message passing rounds\n",
    "        for _ in range(self.no_mp):\n",
    "            x, ea, u = self.meta(x=x, edge_attr=ea, edge_index=ei, u=u, batch=btc)\n",
    "        \n",
    "        #Run the obtained embedding/global frame features through the MLP to obtain final prediction\n",
    "        c = self.mlp(u)\n",
    "        \n",
    "        return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551acab8",
   "metadata": {},
   "source": [
    "# Set up GNN model - GNNCP_ENC (WITH ENCODING!)\n",
    "This GNN model takes input graphs of each frame, and predicts the conformation probability as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "556e3149",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNCP_ENC(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GNNCP_ENC, self).__init__()\n",
    "        \n",
    "        #Define constants for network\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.global_feats = GLOBAL_FEATURES\n",
    "        self.no_mp = NO_MP\n",
    "        \n",
    "        self.mlp_hidden = MLP_HIDDEN\n",
    "        self.targ_size = TARG_SIZE\n",
    "        \n",
    "        self.encoding = ENCODING_SIZE\n",
    "        \n",
    "        #Define MP layer - MetaLayer Class\n",
    "        self.meta = MetaLayer(EdgeModel_ENC(), NodeModel_ENC(), GlobalModel_ENC())\n",
    "        \n",
    "        #Define Encoding MLPs\n",
    "        self.edge_encoding = Seq(Lin(EDGE_FEATURES, self.encoding), LeakyReLU(),\n",
    "                                 Lin(self.encoding, self.encoding)).apply(init_weights)\n",
    "        self.node_encoding = Seq(Lin(NODE_FEATURES, self.encoding), LeakyReLU(),\n",
    "                                 Lin(self.encoding, self.encoding)).apply(init_weights)\n",
    "        \n",
    "        #Define last stage MLP that makes prediction from global\n",
    "        self.mlp = Seq(Lin(self.global_feats, self.mlp_hidden), LeakyReLU(), #BatchNorm1d(self.mlp_hidden), \n",
    "                       Lin(self.mlp_hidden, self.mlp_hidden), LeakyReLU(), #BatchNorm1d(self.mlp_hidden), \n",
    "                       Lin(self.mlp_hidden, self.targ_size)).apply(init_weights)\n",
    "    \n",
    "    def forward(self, g):\n",
    "        #Extract tensors from input graph\n",
    "        x, ea, ei, btc = g.x, g.edge_attr, g.edge_index, g.batch\n",
    "        \n",
    "        #Encode nodes and edges\n",
    "        x = self.node_encoding(x)\n",
    "        ea = self.edge_encoding(ea)\n",
    "        \n",
    "        #Set up global attribute vector for message passing\n",
    "        #careful: has to be dtype torch float 32, and be manually passed to device in case of GPU\n",
    "        u = torch.full(size=(self.batch_size, self.global_feats), fill_value = 1, dtype=torch.float).to(device)\n",
    "        \n",
    "        #Perform the message passing rounds\n",
    "        for _ in range(self.no_mp):\n",
    "            x, ea, u = self.meta(x=x, edge_attr=ea, edge_index=ei, u=u, batch=btc)\n",
    "        \n",
    "        #Run the obtained embedding/global frame features through the MLP to obtain final prediction\n",
    "        c = self.mlp(u)\n",
    "        \n",
    "        return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892a985e",
   "metadata": {},
   "source": [
    "# Set up model parameters and instantiate the GNNCP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "202efe30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda:0\n"
     ]
    }
   ],
   "source": [
    "#Get no. of node and edge features from graphs, as well as target size\n",
    "NODE_FEATURES = list_of_graphs[0].x.size()[1]\n",
    "EDGE_FEATURES = list_of_graphs[0].edge_attr.size()[1]\n",
    "TARG_SIZE = list_of_graphs[0].y.size()[0]\n",
    "\n",
    "ENCODING_SIZE = 16\n",
    "\n",
    "#Set up features\n",
    "HIDDEN_EDGE=16\n",
    "HIDDEN_NODE=16\n",
    "HIDDEN_GLOBAL=16\n",
    "\n",
    "MLP_HIDDEN = 512\n",
    "\n",
    "GLOBAL_FEATURES = 2048\n",
    "NO_MP = 1\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "# --------- Get device to run this on\n",
    "gpu = 0\n",
    "device = torch.device(f\"cuda:{gpu}\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(device)\n",
    "print(f\"DEVICE: {device}\")\n",
    "\n",
    "#Instantiate model\n",
    "#model = GNNCP().to(device)\n",
    "model = GNNCP_ENC().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20d7881b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNNCP_ENC(\n",
       "  (meta): MetaLayer(\n",
       "      edge_model=EdgeModel_ENC(\n",
       "    (edge_mlp): Sequential(\n",
       "      (0): Linear(in_features=48, out_features=16, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "  ),\n",
       "      node_model=NodeModel_ENC(\n",
       "    (node_mlp_1): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=16, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (node_mlp_2): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=16, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "  ),\n",
       "      global_model=GlobalModel_ENC(\n",
       "    (global_mlp): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=16, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=16, out_features=2048, bias=True)\n",
       "    )\n",
       "  )\n",
       "  )\n",
       "  (edge_encoding): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=16, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (node_encoding): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=16, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08deaa54",
   "metadata": {},
   "source": [
    "# Define training function and training parameters, and evaluation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cd282c",
   "metadata": {},
   "source": [
    "## Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f20026bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0001\n",
    "EPOCHS = 2000\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "#criterion = binomial\n",
    "#criterion = torch.nn.MSELoss()\n",
    "#criterion = torch.nn.BCEWithLogitsLoss()\n",
    "mae_loss = torch.nn.L1Loss(reduction=\"mean\")\n",
    "\n",
    "#Register backward hook for gradient clipping\n",
    "clip_value = 0.1\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -clip_value, clip_value))\n",
    "\n",
    "writer = SummaryWriter('runs/GNNCP_MP_'+str(NO_MP)+\"_\"+str(EPOCHS)+\"_epochs\"+\"_\"+str([i for i in time.gmtime()[0:6]]))\n",
    "\n",
    "train_loader = DataLoader(training_data, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(testing_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6b5d82",
   "metadata": {},
   "source": [
    "## Custom loss function - Binomial Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ab5f4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binomial_loss(targ, out, reduce):\n",
    "    term1 = torch.unsqueeze(targ[:,0], dim=1)*torch.log(1+torch.exp(out/reduce))\n",
    "    term1_fix = torch.where(torch.isinf(term1).to(device), torch.tensor([0.]).to(device), term1.to(device))\n",
    "    \n",
    "    term2 = torch.unsqueeze(targ[:,1], dim=1)*torch.log(1+torch.exp(-out/reduce))\n",
    "    term2_fix = torch.where(torch.isinf(term2).to(device), torch.tensor([0.]).to(device), term2.to(device))\n",
    "    \n",
    "    bin_loss = term1_fix + term2_fix\n",
    "    bin_loss = torch.sum(bin_loss)\n",
    "    return bin_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095b848d",
   "metadata": {},
   "source": [
    "# Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5a9c3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#btc=next(iter(train_loader))\n",
    "#out = model(btc.to(device))\n",
    "#out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ffbc8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.exp(out/1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "530038ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1+torch.exp(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cf871a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#binomial_loss(btc.y.to(device),out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3296a963",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "227cdd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(reduce):\n",
    "    loss_avg=[]\n",
    "    for btc_data in train_loader:\n",
    "        #Get target from batch\n",
    "        targ = btc_data.to(device).y\n",
    "        #Forward pass through model\n",
    "        out = model(btc_data.to(device)).T\n",
    "\n",
    "        #loss = criterion(out[0], targ)\n",
    "        loss = binomial_loss(targ, out, reduce)\n",
    "        \n",
    "        #zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_avg.append(loss.item())\n",
    "        \n",
    "        #Empty CUDA cache\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return np.mean(loss_avg)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab24c884",
   "metadata": {},
   "source": [
    "## Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae16036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader, reduce):\n",
    "    error_avg=[]\n",
    "    with torch.no_grad():\n",
    "        for btc_data in loader:\n",
    "            #Get target from batch and send as numpy to cpu\n",
    "            targ = btc_data.y.detach().cpu()\n",
    "            #Forward pass through model\n",
    "            out = model(btc_data.to(device)).T.detach().cpu()\n",
    "            \n",
    "            #Calculate error\n",
    "            #error = mae_loss(out[0], targ)\n",
    "            error = binomial_loss(targ, out, reduce)\n",
    "            error_avg.append(error.item())\n",
    "            \n",
    "            #Empty CUDA cache\n",
    "            torch.cuda.empty_cache()\n",
    "    return np.mean(error_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033270b2",
   "metadata": {},
   "source": [
    "# DOING THE DEED (training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70b9d555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2000 Loss: 1594.4850158691406, Time:1.2s\n",
      "2/2000 Loss: 1611.8314819335938, Time:1.3s\n",
      "3/2000 Loss: 1639.1676330566406, Time:1.29s\n",
      "4/2000 Loss: 1594.778076171875, Time:1.3s\n",
      "5/2000 Loss: 1574.8453369140625, Time:1.3s\n",
      "6/2000 Loss: 1566.251708984375, Time:1.3s\n",
      "7/2000 Loss: 1563.5913696289062, Time:1.3s\n",
      "8/2000 Loss: 1562.862548828125, Time:1.3s\n",
      "9/2000 Loss: 1563.4271240234375, Time:1.3s\n",
      "10/2000 Loss: 1564.0086975097656, Time:1.51s\n",
      "11/2000 Loss: 1564.1086730957031, Time:1.39s\n",
      "12/2000 Loss: 1564.1505126953125, Time:1.31s\n",
      "13/2000 Loss: 1564.170166015625, Time:1.3s\n",
      "14/2000 Loss: 1564.2862854003906, Time:1.3s\n",
      "15/2000 Loss: 1564.5328674316406, Time:1.36s\n",
      "16/2000 Loss: 1564.7257995605469, Time:1.48s\n",
      "17/2000 Loss: 1564.7633666992188, Time:1.35s\n",
      "18/2000 Loss: 1564.7503356933594, Time:1.44s\n",
      "19/2000 Loss: 1564.5555114746094, Time:1.35s\n",
      "20/2000 Loss: 1564.4143371582031, Time:1.34s\n",
      "21/2000 Loss: 1564.3034973144531, Time:1.34s\n",
      "22/2000 Loss: 1564.1260681152344, Time:1.34s\n",
      "23/2000 Loss: 1564.2337951660156, Time:1.35s\n",
      "24/2000 Loss: 1564.2968444824219, Time:1.35s\n",
      "25/2000 Loss: 1564.2398071289062, Time:1.36s\n",
      "26/2000 Loss: 1564.2665710449219, Time:1.34s\n",
      "27/2000 Loss: 1564.2532348632812, Time:1.34s\n",
      "28/2000 Loss: 1564.265869140625, Time:1.35s\n",
      "29/2000 Loss: 1564.3141784667969, Time:1.35s\n",
      "30/2000 Loss: 1564.2611694335938, Time:1.47s\n",
      "31/2000 Loss: 1564.1453552246094, Time:1.34s\n",
      "32/2000 Loss: 1564.038818359375, Time:1.34s\n",
      "33/2000 Loss: 1563.9090881347656, Time:1.34s\n",
      "34/2000 Loss: 1563.805908203125, Time:1.34s\n",
      "35/2000 Loss: 1563.8475952148438, Time:1.35s\n",
      "36/2000 Loss: 1563.7708129882812, Time:1.46s\n",
      "37/2000 Loss: 1563.75244140625, Time:1.35s\n",
      "38/2000 Loss: 1563.658203125, Time:1.3s\n",
      "39/2000 Loss: 1563.5814208984375, Time:1.3s\n",
      "40/2000 Loss: 1563.4519653320312, Time:1.36s\n",
      "41/2000 Loss: 1563.3837585449219, Time:1.34s\n",
      "42/2000 Loss: 1563.4063720703125, Time:1.31s\n",
      "43/2000 Loss: 1563.4837036132812, Time:1.32s\n",
      "44/2000 Loss: 1563.5419006347656, Time:1.39s\n",
      "45/2000 Loss: 1563.6643981933594, Time:1.36s\n",
      "46/2000 Loss: 1563.7325439453125, Time:1.39s\n",
      "47/2000 Loss: 1563.7345886230469, Time:1.33s\n",
      "48/2000 Loss: 1563.647705078125, Time:1.37s\n",
      "49/2000 Loss: 1563.5633544921875, Time:1.63s\n",
      "50/2000 Loss: 1563.5089111328125, Time:1.31s\n",
      "51/2000 Loss: 1563.4546508789062, Time:1.41s\n",
      "52/2000 Loss: 1563.3907165527344, Time:1.34s\n",
      "53/2000 Loss: 1563.2382507324219, Time:1.35s\n",
      "54/2000 Loss: 1563.1256713867188, Time:1.31s\n",
      "55/2000 Loss: 1563.0166015625, Time:1.3s\n",
      "56/2000 Loss: 1562.9317932128906, Time:1.32s\n",
      "57/2000 Loss: 1562.9652709960938, Time:1.31s\n",
      "58/2000 Loss: 1562.9584655761719, Time:1.31s\n",
      "59/2000 Loss: 1562.9661560058594, Time:1.34s\n",
      "60/2000 Loss: 1562.9513549804688, Time:1.33s\n",
      "61/2000 Loss: 1562.9473876953125, Time:1.31s\n",
      "62/2000 Loss: 1562.9236450195312, Time:1.32s\n",
      "63/2000 Loss: 1562.9462280273438, Time:1.3s\n",
      "64/2000 Loss: 1562.9907531738281, Time:1.32s\n",
      "65/2000 Loss: 1562.9753723144531, Time:1.32s\n",
      "66/2000 Loss: 1562.8040161132812, Time:1.34s\n",
      "67/2000 Loss: 1562.7643737792969, Time:1.32s\n",
      "68/2000 Loss: 1562.7738037109375, Time:1.38s\n",
      "69/2000 Loss: 1562.7140502929688, Time:1.4s\n",
      "70/2000 Loss: 1562.5729675292969, Time:1.43s\n",
      "71/2000 Loss: 1562.4794921875, Time:1.54s\n",
      "72/2000 Loss: 1562.343994140625, Time:1.43s\n",
      "73/2000 Loss: 1562.1594543457031, Time:1.53s\n",
      "74/2000 Loss: 1561.96533203125, Time:1.37s\n",
      "75/2000 Loss: 1561.7727661132812, Time:1.33s\n",
      "76/2000 Loss: 1561.6177368164062, Time:1.32s\n",
      "77/2000 Loss: 1561.6216125488281, Time:1.33s\n",
      "78/2000 Loss: 1561.5801086425781, Time:1.35s\n",
      "79/2000 Loss: 1561.5953369140625, Time:1.31s\n",
      "80/2000 Loss: 1561.5794982910156, Time:1.35s\n",
      "81/2000 Loss: 1561.6319580078125, Time:1.34s\n",
      "82/2000 Loss: 1561.6780395507812, Time:1.32s\n",
      "83/2000 Loss: 1561.7035522460938, Time:1.32s\n",
      "84/2000 Loss: 1561.7474975585938, Time:1.41s\n",
      "85/2000 Loss: 1561.7662048339844, Time:1.5s\n",
      "86/2000 Loss: 1561.7300109863281, Time:1.41s\n",
      "87/2000 Loss: 1561.586181640625, Time:1.43s\n",
      "88/2000 Loss: 1561.4542541503906, Time:1.42s\n",
      "89/2000 Loss: 1561.3796691894531, Time:1.36s\n",
      "90/2000 Loss: 1561.2266540527344, Time:1.35s\n",
      "91/2000 Loss: 1561.1141052246094, Time:1.32s\n",
      "92/2000 Loss: 1561.0081176757812, Time:1.31s\n",
      "93/2000 Loss: 1560.9208068847656, Time:1.3s\n",
      "94/2000 Loss: 1560.9120788574219, Time:1.39s\n",
      "95/2000 Loss: 1560.7348937988281, Time:1.36s\n",
      "96/2000 Loss: 1560.599609375, Time:1.31s\n",
      "97/2000 Loss: 1560.5409545898438, Time:1.34s\n",
      "98/2000 Loss: 1560.4613342285156, Time:1.71s\n",
      "99/2000 Loss: 1560.3756713867188, Time:1.45s\n",
      "100/2000 Loss: 1560.3460083007812, Time:1.33s\n",
      "101/2000 Loss: 1560.3121032714844, Time:1.31s\n",
      "102/2000 Loss: 1560.2849426269531, Time:1.31s\n",
      "103/2000 Loss: 1560.2733764648438, Time:1.3s\n",
      "104/2000 Loss: 1560.196044921875, Time:1.34s\n",
      "105/2000 Loss: 1560.1439819335938, Time:1.37s\n",
      "106/2000 Loss: 1559.9701843261719, Time:1.37s\n",
      "107/2000 Loss: 1559.8474426269531, Time:1.42s\n",
      "108/2000 Loss: 1559.6841735839844, Time:1.36s\n",
      "109/2000 Loss: 1559.5782165527344, Time:1.33s\n",
      "110/2000 Loss: 1559.5736389160156, Time:1.42s\n",
      "111/2000 Loss: 1559.5813598632812, Time:1.43s\n",
      "112/2000 Loss: 1559.5775451660156, Time:1.34s\n",
      "113/2000 Loss: 1559.5519409179688, Time:1.39s\n",
      "114/2000 Loss: 1559.3878784179688, Time:1.51s\n",
      "115/2000 Loss: 1559.2909851074219, Time:1.69s\n",
      "116/2000 Loss: 1559.1866455078125, Time:1.37s\n",
      "117/2000 Loss: 1558.9455261230469, Time:1.37s\n",
      "118/2000 Loss: 1558.85009765625, Time:1.38s\n",
      "119/2000 Loss: 1558.7837219238281, Time:1.48s\n",
      "120/2000 Loss: 1558.7388610839844, Time:1.53s\n",
      "121/2000 Loss: 1558.7249145507812, Time:1.35s\n",
      "122/2000 Loss: 1558.6691589355469, Time:1.33s\n",
      "123/2000 Loss: 1558.6468505859375, Time:1.33s\n",
      "124/2000 Loss: 1558.5921936035156, Time:1.32s\n",
      "125/2000 Loss: 1558.5203857421875, Time:1.31s\n",
      "126/2000 Loss: 1558.4801940917969, Time:1.32s\n",
      "127/2000 Loss: 1558.4605407714844, Time:1.33s\n",
      "128/2000 Loss: 1558.3917236328125, Time:1.31s\n",
      "129/2000 Loss: 1558.3337707519531, Time:1.31s\n",
      "130/2000 Loss: 1558.2160949707031, Time:1.31s\n",
      "131/2000 Loss: 1558.0435485839844, Time:1.3s\n",
      "132/2000 Loss: 1557.8881530761719, Time:1.31s\n",
      "133/2000 Loss: 1557.7421875, Time:1.34s\n",
      "134/2000 Loss: 1557.6354370117188, Time:1.32s\n",
      "135/2000 Loss: 1557.5758666992188, Time:1.32s\n",
      "136/2000 Loss: 1557.4796752929688, Time:1.48s\n",
      "137/2000 Loss: 1557.4307861328125, Time:1.32s\n",
      "138/2000 Loss: 1557.4181518554688, Time:1.35s\n",
      "139/2000 Loss: 1557.4097900390625, Time:1.31s\n",
      "140/2000 Loss: 1557.3916015625, Time:1.6s\n",
      "141/2000 Loss: 1557.3626098632812, Time:1.34s\n",
      "142/2000 Loss: 1557.2039184570312, Time:1.47s\n",
      "143/2000 Loss: 1557.0755004882812, Time:1.32s\n",
      "144/2000 Loss: 1556.9934997558594, Time:1.41s\n",
      "145/2000 Loss: 1556.9224853515625, Time:1.32s\n",
      "146/2000 Loss: 1556.7725219726562, Time:1.32s\n",
      "147/2000 Loss: 1556.5779113769531, Time:1.33s\n",
      "148/2000 Loss: 1556.4347534179688, Time:1.34s\n",
      "149/2000 Loss: 1556.30078125, Time:1.38s\n",
      "150/2000 Loss: 1556.2128295898438, Time:1.44s\n",
      "151/2000 Loss: 1556.1615295410156, Time:1.32s\n",
      "152/2000 Loss: 1556.1706848144531, Time:1.35s\n",
      "153/2000 Loss: 1556.1472778320312, Time:1.39s\n",
      "154/2000 Loss: 1556.1130676269531, Time:1.33s\n",
      "155/2000 Loss: 1556.0132446289062, Time:1.38s\n",
      "156/2000 Loss: 1555.9556884765625, Time:1.3s\n",
      "157/2000 Loss: 1555.8932189941406, Time:1.31s\n",
      "158/2000 Loss: 1555.8550720214844, Time:1.31s\n",
      "159/2000 Loss: 1555.8401794433594, Time:1.35s\n",
      "160/2000 Loss: 1555.8363952636719, Time:1.33s\n",
      "161/2000 Loss: 1555.8176879882812, Time:1.31s\n",
      "162/2000 Loss: 1555.7431335449219, Time:1.31s\n",
      "163/2000 Loss: 1555.6760559082031, Time:1.33s\n",
      "164/2000 Loss: 1555.6123657226562, Time:1.31s\n",
      "165/2000 Loss: 1555.5388488769531, Time:1.31s\n",
      "166/2000 Loss: 1555.4681091308594, Time:1.32s\n",
      "167/2000 Loss: 1555.3778686523438, Time:1.32s\n",
      "168/2000 Loss: 1555.2838134765625, Time:1.31s\n",
      "169/2000 Loss: 1555.1938781738281, Time:1.31s\n",
      "170/2000 Loss: 1555.0939331054688, Time:1.31s\n",
      "171/2000 Loss: 1554.9288940429688, Time:1.31s\n",
      "172/2000 Loss: 1554.7572326660156, Time:1.31s\n",
      "173/2000 Loss: 1554.789794921875, Time:1.32s\n",
      "174/2000 Loss: 1554.7650756835938, Time:1.33s\n",
      "175/2000 Loss: 1554.7470397949219, Time:1.31s\n",
      "176/2000 Loss: 1554.6716613769531, Time:1.32s\n",
      "177/2000 Loss: 1554.551025390625, Time:1.39s\n",
      "178/2000 Loss: 1554.3821411132812, Time:1.36s\n",
      "179/2000 Loss: 1554.2011413574219, Time:1.41s\n",
      "180/2000 Loss: 1554.091796875, Time:1.4s\n",
      "181/2000 Loss: 1554.1376037597656, Time:1.41s\n",
      "182/2000 Loss: 1554.1047973632812, Time:1.37s\n",
      "183/2000 Loss: 1554.1495361328125, Time:1.31s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/2000 Loss: 1554.1674499511719, Time:1.35s\n",
      "185/2000 Loss: 1554.0817565917969, Time:1.32s\n",
      "186/2000 Loss: 1553.9241333007812, Time:1.31s\n",
      "187/2000 Loss: 1553.7008972167969, Time:1.33s\n",
      "188/2000 Loss: 1553.4413757324219, Time:1.34s\n",
      "189/2000 Loss: 1553.1502990722656, Time:1.34s\n",
      "190/2000 Loss: 1552.9265747070312, Time:1.31s\n",
      "191/2000 Loss: 1552.7001037597656, Time:1.3s\n",
      "192/2000 Loss: 1552.6172180175781, Time:1.3s\n",
      "193/2000 Loss: 1552.5857849121094, Time:1.31s\n",
      "194/2000 Loss: 1552.5322265625, Time:1.31s\n",
      "195/2000 Loss: 1552.4793701171875, Time:1.3s\n",
      "196/2000 Loss: 1552.4078674316406, Time:1.3s\n",
      "197/2000 Loss: 1552.3325805664062, Time:1.31s\n",
      "198/2000 Loss: 1552.2559814453125, Time:1.31s\n",
      "199/2000 Loss: 1552.2378540039062, Time:1.3s\n",
      "200/2000 Loss: 1552.2151794433594, Time:1.3s\n",
      "201/2000 Loss: 1552.107177734375, Time:1.37s\n",
      "202/2000 Loss: 1552.0025634765625, Time:1.55s\n",
      "203/2000 Loss: 1551.8934631347656, Time:1.35s\n",
      "204/2000 Loss: 1551.8396911621094, Time:1.36s\n",
      "205/2000 Loss: 1551.7563171386719, Time:1.37s\n",
      "206/2000 Loss: 1551.6730651855469, Time:1.41s\n",
      "207/2000 Loss: 1551.5757446289062, Time:1.45s\n",
      "208/2000 Loss: 1551.4962158203125, Time:1.33s\n",
      "209/2000 Loss: 1551.3930053710938, Time:1.31s\n",
      "210/2000 Loss: 1551.2936401367188, Time:1.3s\n",
      "211/2000 Loss: 1551.187255859375, Time:1.33s\n",
      "212/2000 Loss: 1551.09423828125, Time:1.37s\n",
      "213/2000 Loss: 1550.9725952148438, Time:1.32s\n",
      "214/2000 Loss: 1550.8899841308594, Time:1.32s\n",
      "215/2000 Loss: 1550.6756896972656, Time:1.32s\n",
      "216/2000 Loss: 1550.7849426269531, Time:1.33s\n",
      "217/2000 Loss: 1550.7599182128906, Time:1.42s\n",
      "218/2000 Loss: 1550.6332092285156, Time:1.38s\n",
      "219/2000 Loss: 1550.4320068359375, Time:1.34s\n",
      "220/2000 Loss: 1550.6624450683594, Time:1.35s\n",
      "221/2000 Loss: 1550.7250366210938, Time:1.31s\n",
      "222/2000 Loss: 1550.6884155273438, Time:1.33s\n",
      "223/2000 Loss: 1550.5201416015625, Time:1.31s\n",
      "224/2000 Loss: 1550.3468933105469, Time:1.31s\n",
      "225/2000 Loss: 1550.1939086914062, Time:1.31s\n",
      "226/2000 Loss: 1550.0283813476562, Time:1.35s\n",
      "227/2000 Loss: 1549.8946838378906, Time:1.33s\n",
      "228/2000 Loss: 1549.74951171875, Time:1.33s\n",
      "229/2000 Loss: 1549.6284790039062, Time:1.31s\n",
      "230/2000 Loss: 1549.5226745605469, Time:1.3s\n",
      "231/2000 Loss: 1549.4380187988281, Time:1.31s\n",
      "232/2000 Loss: 1549.3590393066406, Time:1.31s\n",
      "233/2000 Loss: 1549.2772216796875, Time:1.31s\n",
      "234/2000 Loss: 1549.1729125976562, Time:1.31s\n",
      "235/2000 Loss: 1549.0696105957031, Time:1.31s\n",
      "236/2000 Loss: 1548.9146118164062, Time:1.37s\n",
      "237/2000 Loss: 1548.8294372558594, Time:1.31s\n",
      "238/2000 Loss: 1548.7931518554688, Time:1.31s\n",
      "239/2000 Loss: 1548.7581481933594, Time:1.31s\n",
      "240/2000 Loss: 1548.7348022460938, Time:1.31s\n",
      "241/2000 Loss: 1548.6738891601562, Time:1.31s\n",
      "242/2000 Loss: 1548.6065368652344, Time:1.31s\n",
      "243/2000 Loss: 1548.5404357910156, Time:1.31s\n",
      "244/2000 Loss: 1548.4461975097656, Time:1.32s\n",
      "245/2000 Loss: 1548.3693237304688, Time:1.31s\n",
      "246/2000 Loss: 1548.3048095703125, Time:1.32s\n",
      "247/2000 Loss: 1548.2315979003906, Time:1.31s\n",
      "248/2000 Loss: 1548.1611328125, Time:1.3s\n",
      "249/2000 Loss: 1548.0863342285156, Time:1.3s\n",
      "250/2000 Loss: 1548.0014953613281, Time:1.3s\n",
      "251/2000 Loss: 1547.9265747070312, Time:1.32s\n",
      "252/2000 Loss: 1547.8396911621094, Time:1.31s\n",
      "253/2000 Loss: 1547.6535339355469, Time:1.31s\n",
      "254/2000 Loss: 1547.4842224121094, Time:1.31s\n",
      "255/2000 Loss: 1547.3816223144531, Time:1.44s\n",
      "256/2000 Loss: 1547.32373046875, Time:1.33s\n",
      "257/2000 Loss: 1547.2770690917969, Time:1.35s\n",
      "258/2000 Loss: 1547.2350769042969, Time:1.36s\n",
      "259/2000 Loss: 1547.1821594238281, Time:1.36s\n",
      "260/2000 Loss: 1547.1253662109375, Time:1.4s\n",
      "261/2000 Loss: 1547.0588684082031, Time:1.45s\n",
      "262/2000 Loss: 1546.9895324707031, Time:1.36s\n",
      "263/2000 Loss: 1546.9193420410156, Time:1.31s\n",
      "264/2000 Loss: 1546.8446044921875, Time:1.3s\n",
      "265/2000 Loss: 1546.7708129882812, Time:1.32s\n",
      "266/2000 Loss: 1546.6916198730469, Time:1.3s\n",
      "267/2000 Loss: 1546.6166381835938, Time:1.3s\n",
      "268/2000 Loss: 1546.4312438964844, Time:1.3s\n",
      "269/2000 Loss: 1546.2990417480469, Time:1.31s\n",
      "270/2000 Loss: 1546.2203979492188, Time:1.38s\n",
      "271/2000 Loss: 1546.1303405761719, Time:1.5s\n",
      "272/2000 Loss: 1546.1204528808594, Time:1.32s\n",
      "273/2000 Loss: 1546.0569152832031, Time:1.3s\n",
      "274/2000 Loss: 1545.9370422363281, Time:1.31s\n",
      "275/2000 Loss: 1545.7837524414062, Time:1.31s\n",
      "276/2000 Loss: 1545.8506164550781, Time:1.31s\n",
      "277/2000 Loss: 1545.8807373046875, Time:1.31s\n",
      "278/2000 Loss: 1545.8027954101562, Time:1.31s\n",
      "279/2000 Loss: 1545.7222900390625, Time:1.31s\n",
      "280/2000 Loss: 1545.6253662109375, Time:1.32s\n",
      "281/2000 Loss: 1545.5179138183594, Time:1.34s\n",
      "282/2000 Loss: 1545.4218444824219, Time:1.31s\n",
      "283/2000 Loss: 1545.3108825683594, Time:1.32s\n",
      "284/2000 Loss: 1545.2476501464844, Time:1.3s\n",
      "285/2000 Loss: 1545.1881103515625, Time:1.35s\n",
      "286/2000 Loss: 1545.1376953125, Time:1.31s\n",
      "287/2000 Loss: 1545.0863037109375, Time:1.29s\n",
      "288/2000 Loss: 1545.0380554199219, Time:1.3s\n",
      "289/2000 Loss: 1544.9873352050781, Time:1.3s\n",
      "290/2000 Loss: 1544.9300231933594, Time:1.31s\n",
      "291/2000 Loss: 1544.8568420410156, Time:1.34s\n",
      "292/2000 Loss: 1544.7782592773438, Time:1.3s\n",
      "293/2000 Loss: 1544.6886901855469, Time:1.33s\n",
      "294/2000 Loss: 1544.6172485351562, Time:1.31s\n",
      "295/2000 Loss: 1544.5703735351562, Time:1.33s\n",
      "296/2000 Loss: 1544.5113830566406, Time:1.32s\n",
      "297/2000 Loss: 1544.4522705078125, Time:1.32s\n",
      "298/2000 Loss: 1544.4063110351562, Time:1.31s\n",
      "299/2000 Loss: 1544.3435974121094, Time:1.31s\n",
      "300/2000 Loss: 1544.2764892578125, Time:1.36s\n",
      "301/2000 Loss: 1544.2073364257812, Time:1.46s\n",
      "302/2000 Loss: 1544.1281127929688, Time:1.51s\n",
      "303/2000 Loss: 1544.0696105957031, Time:1.3s\n",
      "304/2000 Loss: 1543.8888854980469, Time:1.32s\n",
      "305/2000 Loss: 1543.9271850585938, Time:1.31s\n",
      "306/2000 Loss: 1543.9043273925781, Time:1.39s\n",
      "307/2000 Loss: 1543.8145446777344, Time:1.33s\n",
      "308/2000 Loss: 1543.7181396484375, Time:1.32s\n",
      "309/2000 Loss: 1543.6089477539062, Time:1.3s\n",
      "310/2000 Loss: 1543.5061950683594, Time:1.32s\n",
      "311/2000 Loss: 1543.4118041992188, Time:1.31s\n",
      "312/2000 Loss: 1543.3107604980469, Time:1.33s\n",
      "313/2000 Loss: 1543.3814697265625, Time:1.33s\n",
      "314/2000 Loss: 1543.81982421875, Time:1.42s\n",
      "315/2000 Loss: 1543.7439270019531, Time:1.5s\n",
      "316/2000 Loss: 1543.5977172851562, Time:1.32s\n",
      "317/2000 Loss: 1543.3872680664062, Time:1.31s\n",
      "318/2000 Loss: 1543.1966857910156, Time:1.38s\n",
      "319/2000 Loss: 1543.0372314453125, Time:1.77s\n",
      "320/2000 Loss: 1542.9825134277344, Time:1.58s\n",
      "321/2000 Loss: 1542.96142578125, Time:1.32s\n",
      "322/2000 Loss: 1542.953369140625, Time:1.57s\n",
      "323/2000 Loss: 1542.927978515625, Time:1.61s\n",
      "324/2000 Loss: 1542.875732421875, Time:1.32s\n",
      "325/2000 Loss: 1542.8181762695312, Time:1.49s\n",
      "326/2000 Loss: 1542.7594299316406, Time:1.67s\n",
      "327/2000 Loss: 1542.7038879394531, Time:1.61s\n",
      "328/2000 Loss: 1542.6386108398438, Time:1.38s\n",
      "329/2000 Loss: 1542.5736999511719, Time:1.35s\n",
      "330/2000 Loss: 1542.5185852050781, Time:1.37s\n",
      "331/2000 Loss: 1542.4711303710938, Time:1.3s\n",
      "332/2000 Loss: 1542.4225463867188, Time:1.37s\n",
      "333/2000 Loss: 1542.375, Time:1.37s\n",
      "334/2000 Loss: 1542.3131408691406, Time:1.38s\n",
      "335/2000 Loss: 1542.2650451660156, Time:1.42s\n",
      "336/2000 Loss: 1542.2160339355469, Time:1.36s\n",
      "337/2000 Loss: 1542.1744384765625, Time:1.34s\n",
      "338/2000 Loss: 1542.1334838867188, Time:1.34s\n",
      "339/2000 Loss: 1542.0983581542969, Time:1.35s\n",
      "340/2000 Loss: 1542.053466796875, Time:1.42s\n",
      "341/2000 Loss: 1542.01171875, Time:1.39s\n",
      "342/2000 Loss: 1541.9683227539062, Time:1.3s\n",
      "343/2000 Loss: 1541.9271240234375, Time:1.32s\n",
      "344/2000 Loss: 1541.8851013183594, Time:1.31s\n",
      "345/2000 Loss: 1541.8461303710938, Time:1.3s\n",
      "346/2000 Loss: 1541.8088989257812, Time:1.33s\n",
      "347/2000 Loss: 1541.7727355957031, Time:1.36s\n",
      "348/2000 Loss: 1541.7408752441406, Time:1.31s\n",
      "349/2000 Loss: 1541.709716796875, Time:1.31s\n",
      "350/2000 Loss: 1541.6752319335938, Time:1.33s\n",
      "351/2000 Loss: 1541.6337280273438, Time:1.41s\n",
      "352/2000 Loss: 1541.6167602539062, Time:1.67s\n",
      "353/2000 Loss: 1541.5968933105469, Time:1.55s\n",
      "354/2000 Loss: 1541.5774230957031, Time:1.31s\n",
      "355/2000 Loss: 1541.5432434082031, Time:1.36s\n",
      "356/2000 Loss: 1541.5095520019531, Time:1.34s\n",
      "357/2000 Loss: 1541.4844055175781, Time:1.42s\n",
      "358/2000 Loss: 1541.4644165039062, Time:1.43s\n",
      "359/2000 Loss: 1541.4394226074219, Time:1.5s\n",
      "360/2000 Loss: 1541.4276733398438, Time:1.48s\n",
      "361/2000 Loss: 1541.4071044921875, Time:1.53s\n",
      "362/2000 Loss: 1541.3906555175781, Time:1.36s\n",
      "363/2000 Loss: 1541.3793029785156, Time:1.38s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364/2000 Loss: 1541.3724365234375, Time:1.4s\n",
      "365/2000 Loss: 1541.3853759765625, Time:1.3s\n",
      "366/2000 Loss: 1541.3843383789062, Time:1.31s\n",
      "367/2000 Loss: 1541.3648681640625, Time:1.3s\n",
      "368/2000 Loss: 1541.3355407714844, Time:1.31s\n",
      "369/2000 Loss: 1541.3090209960938, Time:1.31s\n",
      "370/2000 Loss: 1541.2988586425781, Time:1.31s\n",
      "371/2000 Loss: 1541.2864990234375, Time:1.31s\n",
      "372/2000 Loss: 1541.271240234375, Time:1.31s\n",
      "373/2000 Loss: 1541.2587585449219, Time:1.32s\n",
      "374/2000 Loss: 1541.2664184570312, Time:1.32s\n",
      "375/2000 Loss: 1541.2706909179688, Time:1.32s\n",
      "376/2000 Loss: 1541.2637023925781, Time:1.33s\n",
      "377/2000 Loss: 1541.29833984375, Time:1.33s\n",
      "378/2000 Loss: 1541.2907409667969, Time:1.34s\n",
      "379/2000 Loss: 1541.2430419921875, Time:1.56s\n",
      "380/2000 Loss: 1541.239501953125, Time:1.34s\n",
      "381/2000 Loss: 1541.2597045898438, Time:1.31s\n",
      "382/2000 Loss: 1541.2816772460938, Time:1.31s\n",
      "383/2000 Loss: 1541.2854309082031, Time:1.34s\n",
      "384/2000 Loss: 1541.2440185546875, Time:1.31s\n",
      "385/2000 Loss: 1541.1870727539062, Time:1.34s\n",
      "386/2000 Loss: 1541.1510314941406, Time:1.31s\n",
      "387/2000 Loss: 1541.1541442871094, Time:1.33s\n",
      "388/2000 Loss: 1541.1582641601562, Time:1.3s\n",
      "389/2000 Loss: 1541.1509094238281, Time:1.33s\n",
      "390/2000 Loss: 1541.1832885742188, Time:1.33s\n",
      "391/2000 Loss: 1541.2189636230469, Time:1.31s\n",
      "392/2000 Loss: 1541.21923828125, Time:1.3s\n",
      "393/2000 Loss: 1541.2134094238281, Time:1.31s\n",
      "394/2000 Loss: 1541.20458984375, Time:1.31s\n",
      "395/2000 Loss: 1541.205078125, Time:1.33s\n",
      "396/2000 Loss: 1541.2003784179688, Time:1.3s\n",
      "397/2000 Loss: 1541.2235412597656, Time:1.32s\n",
      "398/2000 Loss: 1541.2401123046875, Time:1.3s\n",
      "399/2000 Loss: 1541.2726745605469, Time:1.33s\n",
      "400/2000 Loss: 1541.2685241699219, Time:1.35s\n",
      "401/2000 Loss: 1541.3008117675781, Time:1.31s\n",
      "402/2000 Loss: 1541.3197326660156, Time:1.38s\n",
      "403/2000 Loss: 1541.3484802246094, Time:1.32s\n",
      "404/2000 Loss: 1541.3558349609375, Time:1.37s\n",
      "405/2000 Loss: 1541.3323669433594, Time:1.52s\n",
      "406/2000 Loss: 1541.2883911132812, Time:1.38s\n",
      "407/2000 Loss: 1541.3153076171875, Time:1.49s\n",
      "408/2000 Loss: 1541.3777160644531, Time:1.35s\n",
      "409/2000 Loss: 1541.4079284667969, Time:1.35s\n",
      "410/2000 Loss: 1541.3970642089844, Time:1.36s\n",
      "411/2000 Loss: 1541.3578186035156, Time:1.35s\n",
      "412/2000 Loss: 1541.3184204101562, Time:1.38s\n",
      "413/2000 Loss: 1541.3733520507812, Time:1.45s\n",
      "414/2000 Loss: 1541.4370727539062, Time:1.75s\n",
      "415/2000 Loss: 1541.4564819335938, Time:1.35s\n",
      "416/2000 Loss: 1541.4219665527344, Time:1.32s\n",
      "417/2000 Loss: 1541.4185485839844, Time:1.37s\n",
      "418/2000 Loss: 1541.3713684082031, Time:1.38s\n",
      "419/2000 Loss: 1541.3544921875, Time:1.4s\n",
      "420/2000 Loss: 1541.4457092285156, Time:1.37s\n",
      "421/2000 Loss: 1541.440673828125, Time:1.45s\n",
      "422/2000 Loss: 1541.421875, Time:1.44s\n",
      "423/2000 Loss: 1541.3377685546875, Time:1.31s\n",
      "424/2000 Loss: 1541.3604736328125, Time:1.3s\n",
      "425/2000 Loss: 1541.4459228515625, Time:1.4s\n",
      "426/2000 Loss: 1541.435546875, Time:1.76s\n",
      "427/2000 Loss: 1541.4288024902344, Time:1.42s\n",
      "428/2000 Loss: 1541.4316711425781, Time:1.48s\n",
      "429/2000 Loss: 1541.4303588867188, Time:1.6s\n",
      "430/2000 Loss: 1541.4263610839844, Time:1.51s\n",
      "431/2000 Loss: 1541.4068908691406, Time:1.34s\n",
      "432/2000 Loss: 1541.4242248535156, Time:1.33s\n",
      "433/2000 Loss: 1541.4994506835938, Time:1.39s\n",
      "434/2000 Loss: 1541.4010009765625, Time:1.33s\n",
      "435/2000 Loss: 1541.3540954589844, Time:1.35s\n",
      "436/2000 Loss: 1541.4975280761719, Time:1.33s\n",
      "437/2000 Loss: 1541.5417785644531, Time:1.33s\n",
      "438/2000 Loss: 1541.4184875488281, Time:1.37s\n",
      "439/2000 Loss: 1541.4805603027344, Time:1.3s\n",
      "440/2000 Loss: 1541.5668334960938, Time:1.44s\n",
      "441/2000 Loss: 1541.5558776855469, Time:1.44s\n",
      "442/2000 Loss: 1541.5540771484375, Time:1.47s\n",
      "443/2000 Loss: 1541.4879150390625, Time:1.34s\n",
      "444/2000 Loss: 1541.4852905273438, Time:1.45s\n",
      "445/2000 Loss: 1541.5330200195312, Time:1.47s\n",
      "446/2000 Loss: 1541.5041809082031, Time:1.51s\n",
      "447/2000 Loss: 1541.5213928222656, Time:1.32s\n",
      "448/2000 Loss: 1541.5526123046875, Time:1.32s\n",
      "449/2000 Loss: 1541.5065612792969, Time:1.31s\n",
      "450/2000 Loss: 1541.5364379882812, Time:1.29s\n",
      "451/2000 Loss: 1541.5628356933594, Time:1.3s\n",
      "452/2000 Loss: 1541.4843444824219, Time:1.41s\n",
      "453/2000 Loss: 1541.5581665039062, Time:1.47s\n",
      "454/2000 Loss: 1541.5553283691406, Time:1.49s\n",
      "455/2000 Loss: 1541.5247802734375, Time:1.3s\n",
      "456/2000 Loss: 1541.6385498046875, Time:1.35s\n",
      "457/2000 Loss: 1541.5487976074219, Time:1.43s\n",
      "458/2000 Loss: 1541.4911499023438, Time:1.34s\n",
      "459/2000 Loss: 1541.646240234375, Time:1.32s\n",
      "460/2000 Loss: 1541.6877136230469, Time:1.3s\n",
      "461/2000 Loss: 1541.5044250488281, Time:1.3s\n",
      "462/2000 Loss: 1541.4533996582031, Time:1.3s\n",
      "463/2000 Loss: 1541.7207641601562, Time:1.3s\n",
      "464/2000 Loss: 1541.6208190917969, Time:1.39s\n",
      "465/2000 Loss: 1541.6254272460938, Time:1.37s\n",
      "466/2000 Loss: 1541.6722717285156, Time:1.31s\n",
      "467/2000 Loss: 1541.729248046875, Time:1.3s\n",
      "468/2000 Loss: 1541.7124633789062, Time:1.3s\n",
      "469/2000 Loss: 1541.6647644042969, Time:1.35s\n",
      "470/2000 Loss: 1541.7337646484375, Time:1.3s\n",
      "471/2000 Loss: 1541.7660827636719, Time:1.3s\n",
      "472/2000 Loss: 1541.6156616210938, Time:1.37s\n",
      "473/2000 Loss: 1541.536865234375, Time:1.31s\n",
      "474/2000 Loss: 1541.6947937011719, Time:1.3s\n",
      "475/2000 Loss: 1541.7555847167969, Time:1.29s\n",
      "476/2000 Loss: 1541.5700073242188, Time:1.3s\n",
      "477/2000 Loss: 1541.7105102539062, Time:1.44s\n",
      "478/2000 Loss: 1541.7686767578125, Time:1.3s\n",
      "479/2000 Loss: 1541.7157287597656, Time:1.29s\n",
      "480/2000 Loss: 1541.81982421875, Time:1.29s\n",
      "481/2000 Loss: 1541.8595886230469, Time:1.3s\n",
      "482/2000 Loss: 1541.7554626464844, Time:1.29s\n",
      "483/2000 Loss: 1541.7486267089844, Time:1.3s\n",
      "484/2000 Loss: 1541.8042907714844, Time:1.29s\n",
      "485/2000 Loss: 1541.8092041015625, Time:1.3s\n",
      "486/2000 Loss: 1541.7839660644531, Time:1.3s\n",
      "487/2000 Loss: 1541.7833557128906, Time:1.3s\n",
      "488/2000 Loss: 1541.80810546875, Time:1.3s\n",
      "489/2000 Loss: 1541.8035278320312, Time:1.36s\n",
      "490/2000 Loss: 1541.8952331542969, Time:1.29s\n",
      "491/2000 Loss: 1542.0192565917969, Time:1.29s\n",
      "492/2000 Loss: 1541.824462890625, Time:1.3s\n",
      "493/2000 Loss: 1541.9156494140625, Time:1.29s\n",
      "494/2000 Loss: 1541.8697509765625, Time:1.37s\n",
      "495/2000 Loss: 1541.93115234375, Time:1.29s\n",
      "496/2000 Loss: 1542.0152587890625, Time:1.3s\n",
      "497/2000 Loss: 1541.8668518066406, Time:1.29s\n",
      "498/2000 Loss: 1541.9664916992188, Time:1.3s\n",
      "499/2000 Loss: 1541.8593139648438, Time:1.32s\n",
      "500/2000 Loss: 1541.895263671875, Time:1.34s\n",
      "501/2000 Loss: 1541.9930419921875, Time:1.41s\n",
      "502/2000 Loss: 1542.0591125488281, Time:1.35s\n",
      "503/2000 Loss: 1542.0483703613281, Time:1.3s\n",
      "504/2000 Loss: 1542.2030639648438, Time:1.36s\n",
      "505/2000 Loss: 1542.0550537109375, Time:1.37s\n",
      "506/2000 Loss: 1541.9002075195312, Time:1.29s\n",
      "507/2000 Loss: 1542.2154846191406, Time:1.44s\n",
      "508/2000 Loss: 1542.0045166015625, Time:1.31s\n",
      "509/2000 Loss: 1542.1829833984375, Time:1.35s\n",
      "510/2000 Loss: 1542.0526428222656, Time:1.4s\n",
      "511/2000 Loss: 1541.8570861816406, Time:1.35s\n",
      "512/2000 Loss: 1542.2633361816406, Time:1.36s\n",
      "513/2000 Loss: 1541.994384765625, Time:1.4s\n",
      "514/2000 Loss: 1542.0577697753906, Time:1.31s\n",
      "515/2000 Loss: 1542.0941772460938, Time:1.37s\n",
      "516/2000 Loss: 1542.0301208496094, Time:1.58s\n",
      "517/2000 Loss: 1542.1148376464844, Time:1.3s\n",
      "518/2000 Loss: 1542.0991821289062, Time:1.32s\n",
      "519/2000 Loss: 1542.2984008789062, Time:1.3s\n",
      "520/2000 Loss: 1542.1270141601562, Time:1.38s\n",
      "521/2000 Loss: 1542.2269287109375, Time:1.85s\n",
      "522/2000 Loss: 1542.1067810058594, Time:1.53s\n",
      "523/2000 Loss: 1542.2756652832031, Time:1.29s\n",
      "524/2000 Loss: 1542.2244873046875, Time:1.29s\n",
      "525/2000 Loss: 1542.4591674804688, Time:1.3s\n",
      "526/2000 Loss: 1541.9609069824219, Time:1.29s\n",
      "527/2000 Loss: 1543.1076354980469, Time:1.3s\n",
      "528/2000 Loss: 1541.5061340332031, Time:1.29s\n",
      "529/2000 Loss: 1543.1535339355469, Time:1.3s\n",
      "530/2000 Loss: 1568.2152404785156, Time:1.31s\n",
      "531/2000 Loss: 1541.4084167480469, Time:1.35s\n",
      "532/2000 Loss: 1550.6138305664062, Time:1.37s\n",
      "533/2000 Loss: 1557.643310546875, Time:1.45s\n",
      "534/2000 Loss: 1538.8659362792969, Time:1.39s\n",
      "535/2000 Loss: 1555.3583068847656, Time:1.38s\n",
      "536/2000 Loss: 1553.8059997558594, Time:1.39s\n",
      "537/2000 Loss: 1547.8434143066406, Time:1.47s\n",
      "538/2000 Loss: 1590.7322082519531, Time:1.39s\n",
      "539/2000 Loss: 1583.7978515625, Time:1.42s\n",
      "540/2000 Loss: 1547.9025573730469, Time:1.36s\n",
      "541/2000 Loss: 1556.947021484375, Time:1.41s\n",
      "542/2000 Loss: 1556.6463623046875, Time:1.36s\n",
      "543/2000 Loss: 1557.0504760742188, Time:1.38s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544/2000 Loss: 1553.3598327636719, Time:1.38s\n",
      "545/2000 Loss: 1546.2137451171875, Time:1.37s\n",
      "546/2000 Loss: 1541.8599548339844, Time:1.44s\n",
      "547/2000 Loss: 1542.384765625, Time:1.33s\n",
      "548/2000 Loss: 1545.5750732421875, Time:1.3s\n",
      "549/2000 Loss: 1541.4854125976562, Time:1.42s\n",
      "550/2000 Loss: 1551.3176574707031, Time:1.32s\n",
      "551/2000 Loss: 1541.3783874511719, Time:1.42s\n",
      "552/2000 Loss: 1542.1252746582031, Time:1.46s\n",
      "553/2000 Loss: 1550.5324096679688, Time:1.46s\n",
      "554/2000 Loss: 1540.5959777832031, Time:1.54s\n",
      "555/2000 Loss: 1543.8924560546875, Time:1.32s\n",
      "556/2000 Loss: 1552.8282775878906, Time:1.34s\n",
      "557/2000 Loss: 1542.1226196289062, Time:1.32s\n",
      "558/2000 Loss: 1541.3035278320312, Time:1.32s\n",
      "559/2000 Loss: 1545.76611328125, Time:1.39s\n",
      "560/2000 Loss: 1541.1536560058594, Time:1.34s\n",
      "561/2000 Loss: 1549.7939758300781, Time:1.34s\n",
      "562/2000 Loss: 1542.0189819335938, Time:1.41s\n",
      "563/2000 Loss: 1541.4309387207031, Time:1.32s\n",
      "564/2000 Loss: 1543.5367736816406, Time:1.32s\n",
      "565/2000 Loss: 1543.9534606933594, Time:1.44s\n",
      "566/2000 Loss: 1542.9687194824219, Time:1.42s\n",
      "567/2000 Loss: 1542.0543212890625, Time:1.48s\n",
      "568/2000 Loss: 1541.1334228515625, Time:1.85s\n",
      "569/2000 Loss: 1541.4683837890625, Time:1.51s\n",
      "570/2000 Loss: 1542.2578430175781, Time:1.32s\n",
      "571/2000 Loss: 1542.4396057128906, Time:1.32s\n",
      "572/2000 Loss: 1541.9961547851562, Time:1.33s\n",
      "573/2000 Loss: 1541.6001281738281, Time:1.3s\n",
      "574/2000 Loss: 1541.3530578613281, Time:1.31s\n",
      "575/2000 Loss: 1541.2913818359375, Time:1.31s\n",
      "576/2000 Loss: 1541.3794250488281, Time:1.3s\n",
      "577/2000 Loss: 1541.5220336914062, Time:1.31s\n",
      "578/2000 Loss: 1541.5759582519531, Time:1.44s\n",
      "579/2000 Loss: 1541.5351257324219, Time:1.94s\n",
      "580/2000 Loss: 1541.455322265625, Time:1.28s\n",
      "581/2000 Loss: 1541.3807983398438, Time:1.88s\n",
      "582/2000 Loss: 1541.3587951660156, Time:1.44s\n",
      "583/2000 Loss: 1541.3742980957031, Time:1.34s\n",
      "584/2000 Loss: 1541.388671875, Time:1.31s\n",
      "585/2000 Loss: 1541.3995056152344, Time:1.33s\n",
      "586/2000 Loss: 1541.3863220214844, Time:1.33s\n",
      "587/2000 Loss: 1541.3616333007812, Time:1.37s\n",
      "588/2000 Loss: 1541.3372802734375, Time:1.38s\n",
      "589/2000 Loss: 1541.3432922363281, Time:1.34s\n",
      "590/2000 Loss: 1541.3860168457031, Time:1.36s\n",
      "591/2000 Loss: 1541.443115234375, Time:1.31s\n",
      "592/2000 Loss: 1541.4120178222656, Time:1.33s\n",
      "593/2000 Loss: 1541.3609008789062, Time:1.34s\n",
      "594/2000 Loss: 1541.3460998535156, Time:1.36s\n",
      "595/2000 Loss: 1541.3736877441406, Time:1.37s\n",
      "596/2000 Loss: 1541.3782958984375, Time:1.34s\n",
      "597/2000 Loss: 1541.3853759765625, Time:1.34s\n",
      "598/2000 Loss: 1541.4663696289062, Time:1.33s\n",
      "599/2000 Loss: 1541.4847106933594, Time:1.35s\n",
      "600/2000 Loss: 1541.4966735839844, Time:1.37s\n",
      "601/2000 Loss: 1541.4981079101562, Time:1.37s\n",
      "602/2000 Loss: 1541.500732421875, Time:1.39s\n",
      "603/2000 Loss: 1541.5037536621094, Time:1.34s\n",
      "604/2000 Loss: 1541.6212158203125, Time:1.35s\n",
      "605/2000 Loss: 1541.5896606445312, Time:1.35s\n",
      "606/2000 Loss: 1541.6462707519531, Time:1.36s\n",
      "607/2000 Loss: 1541.7215881347656, Time:1.37s\n",
      "608/2000 Loss: 1541.6754760742188, Time:1.37s\n",
      "609/2000 Loss: 1541.57861328125, Time:1.35s\n",
      "610/2000 Loss: 1541.6700134277344, Time:1.35s\n",
      "611/2000 Loss: 1541.6875305175781, Time:1.34s\n",
      "612/2000 Loss: 1541.6460876464844, Time:1.34s\n",
      "613/2000 Loss: 1541.6631469726562, Time:1.35s\n",
      "614/2000 Loss: 1541.6555786132812, Time:1.37s\n",
      "615/2000 Loss: 1541.7527770996094, Time:1.33s\n",
      "616/2000 Loss: 1541.7636108398438, Time:1.36s\n",
      "617/2000 Loss: 1541.7247619628906, Time:1.32s\n",
      "618/2000 Loss: 1541.7699584960938, Time:1.33s\n",
      "619/2000 Loss: 1541.8312377929688, Time:1.34s\n",
      "620/2000 Loss: 1541.8848571777344, Time:1.35s\n",
      "621/2000 Loss: 1541.8753967285156, Time:1.35s\n",
      "622/2000 Loss: 1541.8929748535156, Time:1.35s\n",
      "623/2000 Loss: 1541.8317260742188, Time:1.34s\n",
      "624/2000 Loss: 1541.7811889648438, Time:1.4s\n",
      "625/2000 Loss: 1542.06005859375, Time:1.53s\n",
      "626/2000 Loss: 1541.9815063476562, Time:1.43s\n",
      "627/2000 Loss: 1542.1225891113281, Time:1.53s\n",
      "628/2000 Loss: 1542.17626953125, Time:1.38s\n",
      "629/2000 Loss: 1541.9527893066406, Time:1.35s\n",
      "630/2000 Loss: 1542.0622863769531, Time:1.62s\n",
      "631/2000 Loss: 1542.2248229980469, Time:1.52s\n",
      "632/2000 Loss: 1542.010498046875, Time:1.66s\n",
      "633/2000 Loss: 1542.0658264160156, Time:1.4s\n",
      "634/2000 Loss: 1542.1376342773438, Time:1.38s\n",
      "635/2000 Loss: 1542.0184631347656, Time:1.39s\n",
      "636/2000 Loss: 1542.05712890625, Time:1.39s\n",
      "637/2000 Loss: 1542.1829223632812, Time:1.45s\n",
      "638/2000 Loss: 1541.9957580566406, Time:1.53s\n",
      "639/2000 Loss: 1542.0361328125, Time:1.31s\n",
      "640/2000 Loss: 1542.1048889160156, Time:1.38s\n",
      "641/2000 Loss: 1542.160400390625, Time:1.47s\n",
      "642/2000 Loss: 1542.1212768554688, Time:1.41s\n",
      "643/2000 Loss: 1542.1134338378906, Time:1.33s\n",
      "644/2000 Loss: 1542.1578369140625, Time:1.4s\n",
      "645/2000 Loss: 1542.1541442871094, Time:1.31s\n",
      "646/2000 Loss: 1542.087890625, Time:1.31s\n",
      "647/2000 Loss: 1542.2225341796875, Time:1.35s\n",
      "648/2000 Loss: 1542.1532897949219, Time:1.5s\n",
      "649/2000 Loss: 1541.99560546875, Time:1.34s\n",
      "650/2000 Loss: 1542.17138671875, Time:1.3s\n",
      "651/2000 Loss: 1542.1206970214844, Time:1.33s\n",
      "652/2000 Loss: 1541.975830078125, Time:1.38s\n",
      "653/2000 Loss: 1542.3624877929688, Time:1.37s\n",
      "654/2000 Loss: 1542.2894287109375, Time:1.33s\n",
      "655/2000 Loss: 1541.999267578125, Time:1.38s\n",
      "656/2000 Loss: 1542.2201843261719, Time:1.47s\n",
      "657/2000 Loss: 1542.1807250976562, Time:1.43s\n",
      "658/2000 Loss: 1542.047607421875, Time:1.41s\n",
      "659/2000 Loss: 1542.1798400878906, Time:1.37s\n",
      "660/2000 Loss: 1542.1386413574219, Time:1.5s\n",
      "661/2000 Loss: 1542.1524353027344, Time:1.34s\n",
      "662/2000 Loss: 1542.1755981445312, Time:1.36s\n",
      "663/2000 Loss: 1542.3283386230469, Time:1.36s\n",
      "664/2000 Loss: 1542.2620239257812, Time:1.34s\n",
      "665/2000 Loss: 1542.5528869628906, Time:1.36s\n",
      "666/2000 Loss: 1542.2828063964844, Time:1.45s\n",
      "667/2000 Loss: 1542.4927978515625, Time:1.44s\n",
      "668/2000 Loss: 1542.2707824707031, Time:1.36s\n",
      "669/2000 Loss: 1542.4377746582031, Time:1.35s\n",
      "670/2000 Loss: 1542.2071533203125, Time:1.44s\n",
      "671/2000 Loss: 1542.4974670410156, Time:1.37s\n",
      "672/2000 Loss: 1542.50341796875, Time:1.4s\n",
      "673/2000 Loss: 1542.26953125, Time:1.4s\n",
      "674/2000 Loss: 1542.5318603515625, Time:1.45s\n",
      "675/2000 Loss: 1542.4122924804688, Time:1.4s\n",
      "676/2000 Loss: 1542.4873352050781, Time:1.48s\n",
      "677/2000 Loss: 1542.6262512207031, Time:1.54s\n",
      "678/2000 Loss: 1542.3588256835938, Time:1.5s\n",
      "679/2000 Loss: 1542.5527954101562, Time:1.52s\n",
      "680/2000 Loss: 1542.3766479492188, Time:1.47s\n",
      "681/2000 Loss: 1542.4837646484375, Time:1.41s\n",
      "682/2000 Loss: 1542.4407043457031, Time:1.41s\n",
      "683/2000 Loss: 1542.4379577636719, Time:1.41s\n",
      "684/2000 Loss: 1542.5215759277344, Time:1.43s\n",
      "685/2000 Loss: 1542.604248046875, Time:1.35s\n",
      "686/2000 Loss: 1542.50244140625, Time:1.33s\n",
      "687/2000 Loss: 1542.5466003417969, Time:1.35s\n",
      "688/2000 Loss: 1542.5149841308594, Time:1.34s\n",
      "689/2000 Loss: 1542.4404296875, Time:1.34s\n",
      "690/2000 Loss: 1542.5527954101562, Time:1.42s\n",
      "691/2000 Loss: 1542.3565673828125, Time:1.47s\n",
      "692/2000 Loss: 1542.5445251464844, Time:1.53s\n",
      "693/2000 Loss: 1542.4002075195312, Time:1.37s\n",
      "694/2000 Loss: 1542.32373046875, Time:1.38s\n",
      "695/2000 Loss: 1542.7172546386719, Time:1.44s\n",
      "696/2000 Loss: 1542.2919311523438, Time:1.39s\n",
      "697/2000 Loss: 1542.6144104003906, Time:1.37s\n",
      "698/2000 Loss: 1542.3092651367188, Time:1.54s\n",
      "699/2000 Loss: 1542.4005737304688, Time:1.57s\n",
      "700/2000 Loss: 1542.3391418457031, Time:1.46s\n",
      "701/2000 Loss: 1542.4524841308594, Time:1.39s\n",
      "702/2000 Loss: 1542.4601440429688, Time:1.38s\n",
      "703/2000 Loss: 1542.3828125, Time:1.41s\n",
      "704/2000 Loss: 1542.4730529785156, Time:1.38s\n",
      "705/2000 Loss: 1542.4839782714844, Time:1.46s\n",
      "706/2000 Loss: 1542.364013671875, Time:1.56s\n",
      "707/2000 Loss: 1542.5434265136719, Time:1.43s\n",
      "708/2000 Loss: 1542.33837890625, Time:1.41s\n",
      "709/2000 Loss: 1542.6706237792969, Time:1.54s\n",
      "710/2000 Loss: 1542.1972351074219, Time:1.58s\n",
      "711/2000 Loss: 1542.9515991210938, Time:1.64s\n",
      "712/2000 Loss: 1542.1755676269531, Time:1.57s\n",
      "713/2000 Loss: 1543.1827087402344, Time:1.45s\n",
      "714/2000 Loss: 1542.4162902832031, Time:1.36s\n",
      "715/2000 Loss: 1542.5074768066406, Time:1.52s\n",
      "716/2000 Loss: 1542.6858825683594, Time:1.46s\n",
      "717/2000 Loss: 1542.2159729003906, Time:1.36s\n",
      "718/2000 Loss: 1542.7907104492188, Time:1.42s\n",
      "719/2000 Loss: 1542.281005859375, Time:1.44s\n",
      "720/2000 Loss: 1542.9658203125, Time:1.43s\n",
      "721/2000 Loss: 1542.3136901855469, Time:1.37s\n",
      "722/2000 Loss: 1542.8250732421875, Time:1.49s\n",
      "723/2000 Loss: 1542.2755126953125, Time:1.43s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "724/2000 Loss: 1542.7972717285156, Time:1.44s\n",
      "725/2000 Loss: 1542.5326232910156, Time:1.39s\n",
      "726/2000 Loss: 1542.0955505371094, Time:1.37s\n",
      "727/2000 Loss: 1542.7848815917969, Time:1.38s\n",
      "728/2000 Loss: 1542.4137573242188, Time:1.38s\n",
      "729/2000 Loss: 1542.6904907226562, Time:1.4s\n",
      "730/2000 Loss: 1542.653076171875, Time:1.39s\n",
      "731/2000 Loss: 1542.4156188964844, Time:1.4s\n",
      "732/2000 Loss: 1542.6261291503906, Time:1.38s\n",
      "733/2000 Loss: 1542.7329406738281, Time:1.38s\n",
      "734/2000 Loss: 1542.6493530273438, Time:1.38s\n",
      "735/2000 Loss: 1542.5679931640625, Time:1.37s\n",
      "736/2000 Loss: 1542.9137268066406, Time:1.39s\n",
      "737/2000 Loss: 1542.6365051269531, Time:1.46s\n",
      "738/2000 Loss: 1542.7265014648438, Time:1.49s\n",
      "739/2000 Loss: 1542.7030639648438, Time:1.52s\n",
      "740/2000 Loss: 1542.8128662109375, Time:1.4s\n",
      "741/2000 Loss: 1542.5689392089844, Time:1.41s\n",
      "742/2000 Loss: 1542.89990234375, Time:1.45s\n",
      "743/2000 Loss: 1542.5589294433594, Time:1.43s\n",
      "744/2000 Loss: 1543.2701110839844, Time:1.4s\n",
      "745/2000 Loss: 1542.3750305175781, Time:1.42s\n",
      "746/2000 Loss: 1543.265869140625, Time:1.4s\n",
      "747/2000 Loss: 1542.7122802734375, Time:1.4s\n",
      "748/2000 Loss: 1542.5600280761719, Time:1.38s\n",
      "749/2000 Loss: 1542.9945068359375, Time:1.39s\n",
      "750/2000 Loss: 1542.7016296386719, Time:1.38s\n",
      "751/2000 Loss: 1542.782958984375, Time:1.4s\n",
      "752/2000 Loss: 1542.9839477539062, Time:1.37s\n",
      "753/2000 Loss: 1542.8533020019531, Time:1.39s\n",
      "754/2000 Loss: 1543.0450744628906, Time:1.37s\n",
      "755/2000 Loss: 1542.8338928222656, Time:1.37s\n",
      "756/2000 Loss: 1543.1081237792969, Time:1.38s\n",
      "757/2000 Loss: 1542.9030456542969, Time:1.38s\n",
      "758/2000 Loss: 1543.1968688964844, Time:1.39s\n",
      "759/2000 Loss: 1542.5430908203125, Time:1.37s\n",
      "760/2000 Loss: 1553.4073181152344, Time:1.35s\n",
      "761/2000 Loss: 1540.2784423828125, Time:1.37s\n",
      "762/2000 Loss: 1549.2559204101562, Time:1.34s\n",
      "763/2000 Loss: 1541.0824279785156, Time:1.34s\n",
      "764/2000 Loss: 1551.1411743164062, Time:1.34s\n",
      "765/2000 Loss: 1540.4281311035156, Time:1.35s\n",
      "766/2000 Loss: 1547.039794921875, Time:1.33s\n",
      "767/2000 Loss: 1565.9915161132812, Time:1.38s\n",
      "768/2000 Loss: 1537.9928283691406, Time:1.34s\n",
      "769/2000 Loss: 1561.8349304199219, Time:1.35s\n",
      "770/2000 Loss: 1569.9569702148438, Time:1.35s\n",
      "771/2000 Loss: 1545.16748046875, Time:1.35s\n",
      "772/2000 Loss: 1560.9952697753906, Time:1.34s\n",
      "773/2000 Loss: 1589.0685424804688, Time:1.34s\n",
      "774/2000 Loss: 1608.7241821289062, Time:1.34s\n",
      "775/2000 Loss: 1547.5971069335938, Time:1.35s\n",
      "776/2000 Loss: 1555.3125915527344, Time:1.34s\n",
      "777/2000 Loss: 1552.5804748535156, Time:1.36s\n",
      "778/2000 Loss: 1550.05224609375, Time:1.38s\n",
      "779/2000 Loss: 1547.7560424804688, Time:1.36s\n",
      "780/2000 Loss: 1545.3440856933594, Time:1.35s\n",
      "781/2000 Loss: 1543.3335571289062, Time:1.34s\n",
      "782/2000 Loss: 1541.8017272949219, Time:1.34s\n",
      "783/2000 Loss: 1541.3162841796875, Time:1.35s\n",
      "784/2000 Loss: 1541.6700134277344, Time:1.36s\n",
      "785/2000 Loss: 1542.1872253417969, Time:1.34s\n",
      "786/2000 Loss: 1542.0562744140625, Time:1.36s\n",
      "787/2000 Loss: 1541.631591796875, Time:1.35s\n",
      "788/2000 Loss: 1541.3504028320312, Time:1.34s\n",
      "789/2000 Loss: 1541.3620910644531, Time:1.35s\n",
      "790/2000 Loss: 1541.5056457519531, Time:1.34s\n",
      "791/2000 Loss: 1541.4743041992188, Time:1.37s\n",
      "792/2000 Loss: 1541.3629455566406, Time:1.34s\n",
      "793/2000 Loss: 1541.3527221679688, Time:1.36s\n",
      "794/2000 Loss: 1541.3392028808594, Time:1.35s\n",
      "795/2000 Loss: 1541.3362731933594, Time:1.51s\n",
      "796/2000 Loss: 1541.3727722167969, Time:1.39s\n",
      "797/2000 Loss: 1541.215576171875, Time:1.37s\n",
      "798/2000 Loss: 1541.1744995117188, Time:1.34s\n",
      "799/2000 Loss: 1541.2814025878906, Time:1.36s\n",
      "800/2000 Loss: 1541.3026123046875, Time:1.36s\n",
      "801/2000 Loss: 1541.2466125488281, Time:1.33s\n",
      "802/2000 Loss: 1541.2041931152344, Time:1.34s\n",
      "803/2000 Loss: 1541.3295593261719, Time:1.36s\n",
      "804/2000 Loss: 1541.3864440917969, Time:1.35s\n",
      "805/2000 Loss: 1541.364990234375, Time:1.37s\n",
      "806/2000 Loss: 1541.34814453125, Time:1.36s\n",
      "807/2000 Loss: 1541.3442687988281, Time:1.35s\n",
      "808/2000 Loss: 1541.3123474121094, Time:1.34s\n",
      "809/2000 Loss: 1541.1858825683594, Time:1.35s\n",
      "810/2000 Loss: 1541.1865234375, Time:1.35s\n",
      "811/2000 Loss: 1541.2660217285156, Time:1.34s\n",
      "812/2000 Loss: 1541.3710327148438, Time:1.35s\n",
      "813/2000 Loss: 1541.2352600097656, Time:1.35s\n",
      "814/2000 Loss: 1541.2669372558594, Time:1.34s\n",
      "815/2000 Loss: 1541.3819885253906, Time:1.36s\n",
      "816/2000 Loss: 1541.576904296875, Time:1.35s\n",
      "817/2000 Loss: 1541.6104431152344, Time:1.36s\n",
      "818/2000 Loss: 1541.56201171875, Time:1.33s\n",
      "819/2000 Loss: 1541.5721740722656, Time:1.36s\n",
      "820/2000 Loss: 1541.6021728515625, Time:1.36s\n",
      "821/2000 Loss: 1541.7075500488281, Time:1.35s\n",
      "822/2000 Loss: 1541.7433471679688, Time:1.35s\n",
      "823/2000 Loss: 1541.6607971191406, Time:1.37s\n",
      "824/2000 Loss: 1541.7906494140625, Time:1.37s\n",
      "825/2000 Loss: 1541.7511596679688, Time:1.39s\n",
      "826/2000 Loss: 1541.7637634277344, Time:1.37s\n",
      "827/2000 Loss: 1541.7969665527344, Time:1.36s\n",
      "828/2000 Loss: 1541.912841796875, Time:1.35s\n",
      "829/2000 Loss: 1541.7925109863281, Time:1.34s\n",
      "830/2000 Loss: 1541.6679382324219, Time:1.47s\n",
      "831/2000 Loss: 1542.0492858886719, Time:1.34s\n",
      "832/2000 Loss: 1542.0314331054688, Time:1.4s\n",
      "833/2000 Loss: 1541.8223266601562, Time:1.35s\n",
      "834/2000 Loss: 1542.037109375, Time:1.39s\n",
      "835/2000 Loss: 1542.1347961425781, Time:1.38s\n",
      "836/2000 Loss: 1541.9465637207031, Time:1.43s\n",
      "837/2000 Loss: 1542.1807556152344, Time:1.41s\n",
      "838/2000 Loss: 1542.202880859375, Time:1.36s\n",
      "839/2000 Loss: 1541.998779296875, Time:1.37s\n",
      "840/2000 Loss: 1542.0967712402344, Time:1.34s\n",
      "841/2000 Loss: 1542.1349487304688, Time:1.35s\n",
      "842/2000 Loss: 1542.2535705566406, Time:1.33s\n",
      "843/2000 Loss: 1542.1654357910156, Time:1.34s\n",
      "844/2000 Loss: 1542.22900390625, Time:1.33s\n",
      "845/2000 Loss: 1542.3409423828125, Time:1.34s\n",
      "846/2000 Loss: 1542.1867065429688, Time:1.34s\n",
      "847/2000 Loss: 1542.2966918945312, Time:1.34s\n",
      "848/2000 Loss: 1542.3430480957031, Time:1.34s\n",
      "849/2000 Loss: 1542.2682800292969, Time:1.37s\n",
      "850/2000 Loss: 1542.268798828125, Time:1.35s\n",
      "851/2000 Loss: 1542.4775390625, Time:1.34s\n",
      "852/2000 Loss: 1542.4407348632812, Time:1.33s\n",
      "853/2000 Loss: 1542.3224182128906, Time:1.36s\n",
      "854/2000 Loss: 1542.4059448242188, Time:1.35s\n",
      "855/2000 Loss: 1542.3145446777344, Time:1.35s\n",
      "856/2000 Loss: 1542.4357604980469, Time:1.34s\n",
      "857/2000 Loss: 1542.3829040527344, Time:1.35s\n",
      "858/2000 Loss: 1542.4374389648438, Time:1.34s\n",
      "859/2000 Loss: 1542.6018981933594, Time:1.34s\n",
      "860/2000 Loss: 1542.5916137695312, Time:1.35s\n",
      "861/2000 Loss: 1542.5394287109375, Time:1.35s\n",
      "862/2000 Loss: 1542.5863952636719, Time:1.35s\n",
      "863/2000 Loss: 1542.4118041992188, Time:1.36s\n",
      "864/2000 Loss: 1542.4801330566406, Time:1.34s\n",
      "865/2000 Loss: 1542.7369079589844, Time:1.35s\n",
      "866/2000 Loss: 1542.5743713378906, Time:1.34s\n",
      "867/2000 Loss: 1542.3800048828125, Time:1.36s\n",
      "868/2000 Loss: 1542.713623046875, Time:1.34s\n",
      "869/2000 Loss: 1542.5950622558594, Time:1.35s\n",
      "870/2000 Loss: 1542.4946594238281, Time:1.33s\n",
      "871/2000 Loss: 1542.7652282714844, Time:1.33s\n",
      "872/2000 Loss: 1542.8991394042969, Time:1.33s\n",
      "873/2000 Loss: 1542.7129516601562, Time:1.36s\n",
      "874/2000 Loss: 1542.6334533691406, Time:1.34s\n",
      "875/2000 Loss: 1542.515625, Time:1.36s\n",
      "876/2000 Loss: 1542.8400573730469, Time:1.34s\n",
      "877/2000 Loss: 1542.6116638183594, Time:1.34s\n",
      "878/2000 Loss: 1542.7218322753906, Time:1.33s\n",
      "879/2000 Loss: 1542.8696899414062, Time:1.33s\n",
      "880/2000 Loss: 1542.7039489746094, Time:1.35s\n",
      "881/2000 Loss: 1542.8190002441406, Time:1.35s\n",
      "882/2000 Loss: 1542.8369140625, Time:1.34s\n",
      "883/2000 Loss: 1542.8321838378906, Time:1.34s\n",
      "884/2000 Loss: 1542.7536315917969, Time:1.35s\n",
      "885/2000 Loss: 1542.755126953125, Time:1.34s\n",
      "886/2000 Loss: 1542.8114013671875, Time:1.34s\n",
      "887/2000 Loss: 1542.8646850585938, Time:1.33s\n",
      "888/2000 Loss: 1542.8118286132812, Time:1.36s\n",
      "889/2000 Loss: 1542.8141784667969, Time:1.34s\n",
      "890/2000 Loss: 1542.7991027832031, Time:1.36s\n",
      "891/2000 Loss: 1542.7692565917969, Time:1.41s\n",
      "892/2000 Loss: 1543.0429077148438, Time:1.43s\n",
      "893/2000 Loss: 1542.8840637207031, Time:1.34s\n",
      "894/2000 Loss: 1542.8943481445312, Time:1.35s\n",
      "895/2000 Loss: 1543.0661010742188, Time:1.34s\n",
      "896/2000 Loss: 1542.8659973144531, Time:1.34s\n",
      "897/2000 Loss: 1542.8387145996094, Time:1.34s\n",
      "898/2000 Loss: 1543.0227966308594, Time:1.33s\n",
      "899/2000 Loss: 1542.9512023925781, Time:1.35s\n",
      "900/2000 Loss: 1542.6505126953125, Time:1.34s\n",
      "901/2000 Loss: 1543.27734375, Time:1.35s\n",
      "902/2000 Loss: 1542.5416870117188, Time:1.34s\n",
      "903/2000 Loss: 1543.3700256347656, Time:1.35s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "904/2000 Loss: 1542.3851623535156, Time:1.34s\n",
      "905/2000 Loss: 1543.1076354980469, Time:1.35s\n",
      "906/2000 Loss: 1550.5991516113281, Time:1.35s\n",
      "907/2000 Loss: 1569.5257568359375, Time:1.33s\n",
      "908/2000 Loss: 1547.2981872558594, Time:1.35s\n",
      "909/2000 Loss: 1556.91259765625, Time:1.34s\n",
      "910/2000 Loss: 1573.5970153808594, Time:1.32s\n",
      "911/2000 Loss: 1596.2979431152344, Time:1.34s\n",
      "912/2000 Loss: 1549.8965454101562, Time:1.33s\n",
      "913/2000 Loss: 1543.6995849609375, Time:1.34s\n",
      "914/2000 Loss: 1556.7398681640625, Time:1.33s\n",
      "915/2000 Loss: 1562.0202026367188, Time:1.35s\n",
      "916/2000 Loss: 1539.7957458496094, Time:1.34s\n",
      "917/2000 Loss: 1541.1962280273438, Time:1.34s\n",
      "918/2000 Loss: 1547.8811645507812, Time:1.33s\n",
      "919/2000 Loss: 1610.8070373535156, Time:1.4s\n",
      "920/2000 Loss: 1569.18896484375, Time:1.39s\n",
      "921/2000 Loss: 1600.7100219726562, Time:1.33s\n",
      "922/2000 Loss: 1585.0353393554688, Time:1.34s\n",
      "923/2000 Loss: 1566.40234375, Time:1.34s\n",
      "924/2000 Loss: 1579.9896545410156, Time:1.38s\n",
      "925/2000 Loss: 1544.2693176269531, Time:1.36s\n",
      "926/2000 Loss: 1545.6868896484375, Time:1.51s\n",
      "927/2000 Loss: 1556.6309509277344, Time:1.47s\n",
      "928/2000 Loss: 1545.0579833984375, Time:1.45s\n",
      "929/2000 Loss: 1550.3276977539062, Time:1.49s\n",
      "930/2000 Loss: 1567.489501953125, Time:1.43s\n",
      "931/2000 Loss: 1570.732421875, Time:1.34s\n",
      "932/2000 Loss: 1545.0425415039062, Time:1.35s\n",
      "933/2000 Loss: 1549.0556945800781, Time:1.44s\n",
      "934/2000 Loss: 1556.4687805175781, Time:1.4s\n",
      "935/2000 Loss: 1558.7972412109375, Time:1.36s\n",
      "936/2000 Loss: 1541.0155639648438, Time:1.35s\n",
      "937/2000 Loss: 1548.4833374023438, Time:1.35s\n",
      "938/2000 Loss: 1542.5528259277344, Time:1.48s\n",
      "939/2000 Loss: 1548.8757934570312, Time:1.36s\n",
      "940/2000 Loss: 1571.6465759277344, Time:1.34s\n",
      "941/2000 Loss: 1567.8363647460938, Time:1.34s\n",
      "942/2000 Loss: 1540.6541748046875, Time:1.34s\n",
      "943/2000 Loss: 1551.3113708496094, Time:1.37s\n",
      "944/2000 Loss: 1554.6952514648438, Time:1.34s\n",
      "945/2000 Loss: 1545.5839233398438, Time:1.34s\n",
      "946/2000 Loss: 1549.2954711914062, Time:1.34s\n",
      "947/2000 Loss: 1565.9973449707031, Time:1.35s\n",
      "948/2000 Loss: 1571.3676147460938, Time:1.33s\n",
      "949/2000 Loss: 1544.2269897460938, Time:1.33s\n",
      "950/2000 Loss: 1546.4849243164062, Time:1.32s\n",
      "951/2000 Loss: 1555.0097961425781, Time:1.33s\n",
      "952/2000 Loss: 1555.82666015625, Time:1.34s\n",
      "953/2000 Loss: 1539.8976440429688, Time:1.34s\n",
      "954/2000 Loss: 1552.1895446777344, Time:1.31s\n",
      "955/2000 Loss: 1552.987548828125, Time:1.33s\n",
      "956/2000 Loss: 1539.8021850585938, Time:1.34s\n",
      "957/2000 Loss: 1550.7833557128906, Time:1.35s\n",
      "958/2000 Loss: 1549.9689331054688, Time:1.33s\n",
      "959/2000 Loss: 1539.9367370605469, Time:1.33s\n",
      "960/2000 Loss: 1551.9578857421875, Time:1.32s\n",
      "961/2000 Loss: 1554.0751953125, Time:1.32s\n",
      "962/2000 Loss: 1538.9862365722656, Time:1.31s\n",
      "963/2000 Loss: 1550.5566101074219, Time:1.31s\n",
      "964/2000 Loss: 1550.1589660644531, Time:1.31s\n",
      "965/2000 Loss: 1540.3540344238281, Time:1.32s\n",
      "966/2000 Loss: 1542.3929748535156, Time:1.31s\n",
      "967/2000 Loss: 1541.7488708496094, Time:1.31s\n",
      "968/2000 Loss: 1555.2002563476562, Time:1.32s\n",
      "969/2000 Loss: 1564.1848754882812, Time:1.32s\n",
      "970/2000 Loss: 1540.2242126464844, Time:1.3s\n",
      "971/2000 Loss: 1543.59326171875, Time:1.3s\n",
      "972/2000 Loss: 1542.461181640625, Time:1.32s\n",
      "973/2000 Loss: 1541.1067199707031, Time:1.32s\n",
      "974/2000 Loss: 1539.7212219238281, Time:1.31s\n",
      "975/2000 Loss: 1540.0535278320312, Time:1.31s\n",
      "976/2000 Loss: 1541.5877075195312, Time:1.32s\n",
      "977/2000 Loss: 1541.4918518066406, Time:1.31s\n",
      "978/2000 Loss: 1540.2716674804688, Time:1.31s\n",
      "979/2000 Loss: 1540.2776184082031, Time:1.31s\n",
      "980/2000 Loss: 1541.075439453125, Time:1.32s\n",
      "981/2000 Loss: 1541.1888427734375, Time:1.31s\n",
      "982/2000 Loss: 1540.5502624511719, Time:1.31s\n",
      "983/2000 Loss: 1540.5556335449219, Time:1.32s\n",
      "984/2000 Loss: 1540.8927917480469, Time:1.31s\n",
      "985/2000 Loss: 1540.9259643554688, Time:1.32s\n",
      "986/2000 Loss: 1540.7174682617188, Time:1.33s\n",
      "987/2000 Loss: 1540.7537231445312, Time:1.32s\n",
      "988/2000 Loss: 1540.8587036132812, Time:1.32s\n",
      "989/2000 Loss: 1540.8551330566406, Time:1.31s\n",
      "990/2000 Loss: 1540.8572387695312, Time:1.31s\n",
      "991/2000 Loss: 1540.8653259277344, Time:1.31s\n",
      "992/2000 Loss: 1540.8458862304688, Time:1.33s\n",
      "993/2000 Loss: 1541.0753479003906, Time:1.32s\n",
      "994/2000 Loss: 1540.9043273925781, Time:1.31s\n",
      "995/2000 Loss: 1540.9246826171875, Time:1.32s\n",
      "996/2000 Loss: 1541.0416564941406, Time:1.34s\n",
      "997/2000 Loss: 1541.0117797851562, Time:1.31s\n",
      "998/2000 Loss: 1541.1467895507812, Time:1.31s\n",
      "999/2000 Loss: 1541.3486328125, Time:1.32s\n",
      "1000/2000 Loss: 1541.5765991210938, Time:1.32s\n",
      "1001/2000 Loss: 1541.6588745117188, Time:1.31s\n",
      "1002/2000 Loss: 1541.6620178222656, Time:1.32s\n",
      "1003/2000 Loss: 1541.8460998535156, Time:1.35s\n",
      "1004/2000 Loss: 1541.9722595214844, Time:1.31s\n",
      "1005/2000 Loss: 1541.9525756835938, Time:1.31s\n",
      "1006/2000 Loss: 1541.9658813476562, Time:1.3s\n",
      "1007/2000 Loss: 1542.0541381835938, Time:1.31s\n",
      "1008/2000 Loss: 1542.0577087402344, Time:1.31s\n",
      "1009/2000 Loss: 1542.1674194335938, Time:1.31s\n",
      "1010/2000 Loss: 1542.1810607910156, Time:1.32s\n",
      "1011/2000 Loss: 1542.0794982910156, Time:1.31s\n",
      "1012/2000 Loss: 1542.1206359863281, Time:1.3s\n",
      "1013/2000 Loss: 1542.4046630859375, Time:1.32s\n",
      "1014/2000 Loss: 1542.2798767089844, Time:1.3s\n",
      "1015/2000 Loss: 1542.1336364746094, Time:1.33s\n",
      "1016/2000 Loss: 1542.363037109375, Time:1.31s\n",
      "1017/2000 Loss: 1542.3221435546875, Time:1.32s\n",
      "1018/2000 Loss: 1542.2135925292969, Time:1.31s\n",
      "1019/2000 Loss: 1542.5927429199219, Time:1.3s\n",
      "1020/2000 Loss: 1542.084716796875, Time:1.31s\n",
      "1021/2000 Loss: 1543.0634460449219, Time:1.3s\n",
      "1022/2000 Loss: 1541.7629089355469, Time:1.3s\n",
      "1023/2000 Loss: 1545.4562377929688, Time:1.33s\n",
      "1024/2000 Loss: 1544.4164428710938, Time:1.31s\n",
      "1025/2000 Loss: 1561.0957336425781, Time:1.3s\n",
      "1026/2000 Loss: 1567.4968566894531, Time:1.31s\n",
      "1027/2000 Loss: 1540.4673156738281, Time:1.32s\n",
      "1028/2000 Loss: 1551.5546875, Time:1.3s\n",
      "1029/2000 Loss: 1547.7010498046875, Time:1.31s\n",
      "1030/2000 Loss: 1543.4621276855469, Time:1.31s\n",
      "1031/2000 Loss: 1591.0877075195312, Time:1.32s\n",
      "1032/2000 Loss: 1553.8948974609375, Time:1.3s\n",
      "1033/2000 Loss: 1561.5107421875, Time:1.31s\n",
      "1034/2000 Loss: 1580.7813110351562, Time:1.3s\n",
      "1035/2000 Loss: 1584.3487243652344, Time:1.31s\n",
      "1036/2000 Loss: 1545.7285461425781, Time:1.31s\n",
      "1037/2000 Loss: 1550.5320739746094, Time:1.3s\n",
      "1038/2000 Loss: 1577.0145874023438, Time:1.32s\n",
      "1039/2000 Loss: 1575.8498840332031, Time:1.3s\n",
      "1040/2000 Loss: 1544.5688171386719, Time:1.31s\n",
      "1041/2000 Loss: 1551.0282592773438, Time:1.3s\n",
      "1042/2000 Loss: 1574.5563354492188, Time:1.32s\n",
      "1043/2000 Loss: 1573.4936828613281, Time:1.3s\n",
      "1044/2000 Loss: 1543.4836120605469, Time:1.32s\n",
      "1045/2000 Loss: 1550.8748168945312, Time:1.31s\n",
      "1046/2000 Loss: 1549.9457092285156, Time:1.32s\n",
      "1047/2000 Loss: 1546.5438537597656, Time:1.32s\n",
      "1048/2000 Loss: 1540.6355590820312, Time:1.31s\n",
      "1049/2000 Loss: 1543.6170043945312, Time:1.3s\n",
      "1050/2000 Loss: 1554.3480834960938, Time:1.31s\n",
      "1051/2000 Loss: 1537.7208251953125, Time:1.32s\n",
      "1052/2000 Loss: 1546.9298706054688, Time:1.31s\n",
      "1053/2000 Loss: 1542.9682006835938, Time:1.3s\n",
      "1054/2000 Loss: 1541.1468200683594, Time:1.32s\n",
      "1055/2000 Loss: 1540.0553283691406, Time:1.3s\n",
      "1056/2000 Loss: 1540.3462219238281, Time:1.31s\n",
      "1057/2000 Loss: 1541.4852600097656, Time:1.3s\n",
      "1058/2000 Loss: 1541.4015502929688, Time:1.32s\n",
      "1059/2000 Loss: 1540.4565734863281, Time:1.31s\n",
      "1060/2000 Loss: 1540.306884765625, Time:1.3s\n",
      "1061/2000 Loss: 1540.7875061035156, Time:1.31s\n",
      "1062/2000 Loss: 1541.0212707519531, Time:1.31s\n",
      "1063/2000 Loss: 1540.9856872558594, Time:1.3s\n",
      "1064/2000 Loss: 1540.7543029785156, Time:1.31s\n",
      "1065/2000 Loss: 1540.6315002441406, Time:1.32s\n",
      "1066/2000 Loss: 1540.7142028808594, Time:1.31s\n",
      "1067/2000 Loss: 1540.77294921875, Time:1.3s\n",
      "1068/2000 Loss: 1540.7019958496094, Time:1.29s\n",
      "1069/2000 Loss: 1540.9441528320312, Time:1.31s\n",
      "1070/2000 Loss: 1540.9501037597656, Time:1.43s\n",
      "1071/2000 Loss: 1540.8028564453125, Time:1.31s\n",
      "1072/2000 Loss: 1541.1342163085938, Time:1.31s\n",
      "1073/2000 Loss: 1541.4750671386719, Time:1.31s\n",
      "1074/2000 Loss: 1541.4488830566406, Time:1.32s\n",
      "1075/2000 Loss: 1541.3475341796875, Time:1.31s\n",
      "1076/2000 Loss: 1541.2139587402344, Time:1.3s\n",
      "1077/2000 Loss: 1541.3676147460938, Time:1.31s\n",
      "1078/2000 Loss: 1541.4476318359375, Time:1.32s\n",
      "1079/2000 Loss: 1541.5508422851562, Time:1.3s\n",
      "1080/2000 Loss: 1541.2786560058594, Time:1.3s\n",
      "1081/2000 Loss: 1541.3973083496094, Time:1.33s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1082/2000 Loss: 1541.6876831054688, Time:1.32s\n",
      "1083/2000 Loss: 1541.7046203613281, Time:1.32s\n",
      "1084/2000 Loss: 1541.5296020507812, Time:1.32s\n",
      "1085/2000 Loss: 1541.4606323242188, Time:1.31s\n",
      "1086/2000 Loss: 1541.5367126464844, Time:1.32s\n",
      "1087/2000 Loss: 1541.5196838378906, Time:1.3s\n",
      "1088/2000 Loss: 1541.7386779785156, Time:1.31s\n",
      "1089/2000 Loss: 1541.7394714355469, Time:1.32s\n",
      "1090/2000 Loss: 1541.688232421875, Time:1.31s\n",
      "1091/2000 Loss: 1541.7452392578125, Time:1.29s\n",
      "1092/2000 Loss: 1541.6181030273438, Time:1.31s\n",
      "1093/2000 Loss: 1541.8115234375, Time:1.36s\n",
      "1094/2000 Loss: 1541.9235534667969, Time:1.35s\n",
      "1095/2000 Loss: 1541.6665649414062, Time:1.33s\n",
      "1096/2000 Loss: 1541.813720703125, Time:1.33s\n",
      "1097/2000 Loss: 1541.9123229980469, Time:1.34s\n",
      "1098/2000 Loss: 1541.6661987304688, Time:1.33s\n",
      "1099/2000 Loss: 1541.9570617675781, Time:1.33s\n",
      "1100/2000 Loss: 1541.9320068359375, Time:1.32s\n",
      "1101/2000 Loss: 1541.6305236816406, Time:1.34s\n",
      "1102/2000 Loss: 1541.8283081054688, Time:1.32s\n",
      "1103/2000 Loss: 1541.9503479003906, Time:1.32s\n",
      "1104/2000 Loss: 1541.8564453125, Time:1.32s\n",
      "1105/2000 Loss: 1541.8072814941406, Time:1.33s\n",
      "1106/2000 Loss: 1541.9463500976562, Time:1.32s\n",
      "1107/2000 Loss: 1541.8926391601562, Time:1.31s\n",
      "1108/2000 Loss: 1542.0753784179688, Time:1.33s\n",
      "1109/2000 Loss: 1541.9420471191406, Time:1.31s\n",
      "1110/2000 Loss: 1542.2509460449219, Time:1.32s\n",
      "1111/2000 Loss: 1541.904296875, Time:1.32s\n",
      "1112/2000 Loss: 1542.3075866699219, Time:1.32s\n",
      "1113/2000 Loss: 1542.0076904296875, Time:1.33s\n",
      "1114/2000 Loss: 1541.9110717773438, Time:1.31s\n",
      "1115/2000 Loss: 1542.3456726074219, Time:1.31s\n",
      "1116/2000 Loss: 1542.0531311035156, Time:1.3s\n",
      "1117/2000 Loss: 1541.7438354492188, Time:1.33s\n",
      "1118/2000 Loss: 1541.9505310058594, Time:1.32s\n",
      "1119/2000 Loss: 1541.8775024414062, Time:1.32s\n",
      "1120/2000 Loss: 1542.0358276367188, Time:1.32s\n",
      "1121/2000 Loss: 1542.0912170410156, Time:1.32s\n",
      "1122/2000 Loss: 1541.8488464355469, Time:1.32s\n",
      "1123/2000 Loss: 1542.1171569824219, Time:1.32s\n",
      "1124/2000 Loss: 1542.2593078613281, Time:1.32s\n",
      "1125/2000 Loss: 1542.159423828125, Time:1.31s\n",
      "1126/2000 Loss: 1542.1756896972656, Time:1.31s\n",
      "1127/2000 Loss: 1542.0304260253906, Time:1.32s\n",
      "1128/2000 Loss: 1542.1603393554688, Time:1.31s\n",
      "1129/2000 Loss: 1542.0439453125, Time:1.32s\n",
      "1130/2000 Loss: 1542.3557434082031, Time:1.32s\n",
      "1131/2000 Loss: 1542.0368041992188, Time:1.32s\n",
      "1132/2000 Loss: 1542.0686950683594, Time:1.33s\n",
      "1133/2000 Loss: 1542.1951293945312, Time:1.32s\n",
      "1134/2000 Loss: 1542.2245178222656, Time:1.32s\n",
      "1135/2000 Loss: 1541.8921203613281, Time:1.31s\n",
      "1136/2000 Loss: 1542.3478088378906, Time:1.34s\n",
      "1137/2000 Loss: 1541.9701843261719, Time:1.31s\n",
      "1138/2000 Loss: 1542.7788696289062, Time:1.32s\n",
      "1139/2000 Loss: 1541.8153686523438, Time:1.34s\n",
      "1140/2000 Loss: 1540.2880859375, Time:1.32s\n",
      "1141/2000 Loss: 1549.164306640625, Time:1.31s\n",
      "1142/2000 Loss: 1603.2781066894531, Time:1.31s\n",
      "1143/2000 Loss: 1565.8302612304688, Time:1.3s\n",
      "1144/2000 Loss: 1572.6014709472656, Time:1.31s\n",
      "1145/2000 Loss: 1568.6458435058594, Time:1.31s\n",
      "1146/2000 Loss: 1564.974365234375, Time:1.32s\n",
      "1147/2000 Loss: 1575.4017028808594, Time:1.32s\n",
      "1148/2000 Loss: 1549.8347473144531, Time:1.32s\n",
      "1149/2000 Loss: 1544.8017578125, Time:1.33s\n",
      "1150/2000 Loss: 1554.1761169433594, Time:1.31s\n",
      "1151/2000 Loss: 1553.2201843261719, Time:1.32s\n",
      "1152/2000 Loss: 1541.7010803222656, Time:1.34s\n",
      "1153/2000 Loss: 1552.29296875, Time:1.31s\n",
      "1154/2000 Loss: 1550.7815551757812, Time:1.31s\n",
      "1155/2000 Loss: 1540.9351806640625, Time:1.32s\n",
      "1156/2000 Loss: 1551.0095520019531, Time:1.32s\n",
      "1157/2000 Loss: 1548.8915405273438, Time:1.31s\n",
      "1158/2000 Loss: 1540.2027587890625, Time:1.32s\n",
      "1159/2000 Loss: 1550.35986328125, Time:1.34s\n",
      "1160/2000 Loss: 1549.1181030273438, Time:1.42s\n",
      "1161/2000 Loss: 1539.6468200683594, Time:1.35s\n",
      "1162/2000 Loss: 1549.8200378417969, Time:1.37s\n",
      "1163/2000 Loss: 1548.683349609375, Time:1.38s\n",
      "1164/2000 Loss: 1539.5687255859375, Time:1.42s\n",
      "1165/2000 Loss: 1549.904296875, Time:1.33s\n",
      "1166/2000 Loss: 1549.5677490234375, Time:1.35s\n",
      "1167/2000 Loss: 1539.2388000488281, Time:1.33s\n",
      "1168/2000 Loss: 1549.4799194335938, Time:1.32s\n",
      "1169/2000 Loss: 1548.7484436035156, Time:1.34s\n",
      "1170/2000 Loss: 1539.6138916015625, Time:1.33s\n",
      "1171/2000 Loss: 1550.663330078125, Time:1.34s\n",
      "1172/2000 Loss: 1552.2853698730469, Time:1.33s\n",
      "1173/2000 Loss: 1538.7370910644531, Time:1.34s\n",
      "1174/2000 Loss: 1549.2430725097656, Time:1.33s\n",
      "1175/2000 Loss: 1548.1054382324219, Time:1.34s\n",
      "1176/2000 Loss: 1540.236572265625, Time:1.33s\n",
      "1177/2000 Loss: 1542.3485717773438, Time:1.32s\n",
      "1178/2000 Loss: 1540.9867858886719, Time:1.34s\n",
      "1179/2000 Loss: 1553.3430480957031, Time:1.33s\n",
      "1180/2000 Loss: 1559.8327941894531, Time:1.34s\n",
      "1181/2000 Loss: 1539.2060241699219, Time:1.33s\n",
      "1182/2000 Loss: 1547.3142700195312, Time:1.32s\n",
      "1183/2000 Loss: 1542.5944519042969, Time:1.33s\n",
      "1184/2000 Loss: 1546.5297241210938, Time:1.34s\n",
      "1185/2000 Loss: 1568.2004699707031, Time:1.32s\n",
      "1186/2000 Loss: 1566.3616638183594, Time:1.4s\n",
      "1187/2000 Loss: 1538.9634704589844, Time:1.54s\n",
      "1188/2000 Loss: 1548.3400268554688, Time:1.36s\n",
      "1189/2000 Loss: 1541.9275207519531, Time:1.32s\n",
      "1190/2000 Loss: 1546.3907470703125, Time:1.33s\n",
      "1191/2000 Loss: 1570.0501403808594, Time:1.32s\n",
      "1192/2000 Loss: 1568.7898559570312, Time:1.47s\n",
      "1193/2000 Loss: 1539.529296875, Time:1.32s\n",
      "1194/2000 Loss: 1548.5642700195312, Time:1.32s\n",
      "1195/2000 Loss: 1543.009033203125, Time:1.46s\n",
      "1196/2000 Loss: 1544.9914855957031, Time:1.45s\n",
      "1197/2000 Loss: 1570.2176818847656, Time:1.34s\n",
      "1198/2000 Loss: 1566.5552673339844, Time:1.33s\n",
      "1199/2000 Loss: 1539.6373901367188, Time:1.33s\n",
      "1200/2000 Loss: 1548.7138977050781, Time:1.47s\n",
      "1201/2000 Loss: 1549.5369567871094, Time:1.46s\n",
      "1202/2000 Loss: 1540.3279113769531, Time:1.44s\n",
      "1203/2000 Loss: 1556.9266662597656, Time:1.37s\n",
      "1204/2000 Loss: 1558.9652099609375, Time:1.37s\n",
      "1205/2000 Loss: 1552.2142333984375, Time:1.36s\n",
      "1206/2000 Loss: 1541.0184936523438, Time:1.47s\n",
      "1207/2000 Loss: 1552.3417358398438, Time:1.48s\n",
      "1208/2000 Loss: 1554.6440734863281, Time:1.33s\n",
      "1209/2000 Loss: 1539.0976867675781, Time:1.42s\n",
      "1210/2000 Loss: 1548.4383239746094, Time:1.41s\n",
      "1211/2000 Loss: 1544.8100891113281, Time:1.31s\n",
      "1212/2000 Loss: 1542.1864318847656, Time:1.37s\n",
      "1213/2000 Loss: 1566.9129943847656, Time:1.46s\n",
      "1214/2000 Loss: 1560.68505859375, Time:1.31s\n",
      "1215/2000 Loss: 1539.9531555175781, Time:1.33s\n",
      "1216/2000 Loss: 1551.652587890625, Time:1.32s\n",
      "1217/2000 Loss: 1553.2626953125, Time:1.31s\n",
      "1218/2000 Loss: 1539.1011047363281, Time:1.32s\n",
      "1219/2000 Loss: 1548.0421752929688, Time:1.33s\n",
      "1220/2000 Loss: 1543.7449645996094, Time:1.31s\n",
      "1221/2000 Loss: 1542.9253234863281, Time:1.32s\n",
      "1222/2000 Loss: 1574.7030944824219, Time:1.33s\n",
      "1223/2000 Loss: 1573.5691833496094, Time:1.32s\n",
      "1224/2000 Loss: 1539.0969848632812, Time:1.33s\n",
      "1225/2000 Loss: 1548.026611328125, Time:1.33s\n",
      "1226/2000 Loss: 1540.8176879882812, Time:1.32s\n",
      "1227/2000 Loss: 1546.4369812011719, Time:1.32s\n",
      "1228/2000 Loss: 1574.6288146972656, Time:1.32s\n",
      "1229/2000 Loss: 1576.7011108398438, Time:1.32s\n",
      "1230/2000 Loss: 1539.8608703613281, Time:1.31s\n",
      "1231/2000 Loss: 1545.9854431152344, Time:1.31s\n",
      "1232/2000 Loss: 1545.6768188476562, Time:1.34s\n",
      "1233/2000 Loss: 1537.4778747558594, Time:1.31s\n",
      "1234/2000 Loss: 1544.0838623046875, Time:1.31s\n",
      "1235/2000 Loss: 1541.2341918945312, Time:1.31s\n",
      "1236/2000 Loss: 1537.3377380371094, Time:1.33s\n",
      "1237/2000 Loss: 1542.9916076660156, Time:1.31s\n",
      "1238/2000 Loss: 1539.4587707519531, Time:1.32s\n",
      "1239/2000 Loss: 1543.6708984375, Time:1.32s\n",
      "1240/2000 Loss: 1539.5203857421875, Time:1.33s\n",
      "1241/2000 Loss: 1543.6883239746094, Time:1.33s\n",
      "1242/2000 Loss: 1539.5671081542969, Time:1.32s\n",
      "1243/2000 Loss: 1543.7510986328125, Time:1.32s\n",
      "1244/2000 Loss: 1539.588623046875, Time:1.32s\n",
      "1245/2000 Loss: 1543.6284790039062, Time:1.32s\n",
      "1246/2000 Loss: 1539.622314453125, Time:1.32s\n",
      "1247/2000 Loss: 1543.7744750976562, Time:1.33s\n",
      "1248/2000 Loss: 1539.4396362304688, Time:1.31s\n",
      "1249/2000 Loss: 1543.3240661621094, Time:1.32s\n",
      "1250/2000 Loss: 1539.7615966796875, Time:1.32s\n",
      "1251/2000 Loss: 1543.9804077148438, Time:1.32s\n",
      "1252/2000 Loss: 1539.3172607421875, Time:1.32s\n",
      "1253/2000 Loss: 1542.6588134765625, Time:1.32s\n",
      "1254/2000 Loss: 1540.3555297851562, Time:1.32s\n",
      "1255/2000 Loss: 1547.1253662109375, Time:1.33s\n",
      "1256/2000 Loss: 1538.9366455078125, Time:1.32s\n",
      "1257/2000 Loss: 1543.1358642578125, Time:1.31s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1258/2000 Loss: 1536.8396606445312, Time:1.32s\n",
      "1259/2000 Loss: 1545.4098205566406, Time:1.33s\n",
      "1260/2000 Loss: 1537.2377319335938, Time:1.32s\n",
      "1261/2000 Loss: 1544.9703369140625, Time:1.32s\n",
      "1262/2000 Loss: 1537.2240600585938, Time:1.32s\n",
      "1263/2000 Loss: 1544.1602783203125, Time:1.34s\n",
      "1264/2000 Loss: 1537.3099365234375, Time:1.32s\n",
      "1265/2000 Loss: 1543.9110717773438, Time:1.32s\n",
      "1266/2000 Loss: 1537.2340393066406, Time:1.33s\n",
      "1267/2000 Loss: 1544.46484375, Time:1.33s\n",
      "1268/2000 Loss: 1537.5188903808594, Time:1.32s\n",
      "1269/2000 Loss: 1544.0718383789062, Time:1.32s\n",
      "1270/2000 Loss: 1537.3128051757812, Time:1.33s\n",
      "1271/2000 Loss: 1544.3636779785156, Time:1.33s\n",
      "1272/2000 Loss: 1537.3631286621094, Time:1.33s\n",
      "1273/2000 Loss: 1544.209716796875, Time:1.32s\n",
      "1274/2000 Loss: 1537.3342895507812, Time:1.32s\n",
      "1275/2000 Loss: 1544.1400451660156, Time:1.32s\n",
      "1276/2000 Loss: 1537.2737731933594, Time:1.32s\n",
      "1277/2000 Loss: 1544.1651611328125, Time:1.32s\n",
      "1278/2000 Loss: 1537.2179870605469, Time:1.34s\n",
      "1279/2000 Loss: 1544.1720275878906, Time:1.31s\n",
      "1280/2000 Loss: 1537.1414184570312, Time:1.32s\n",
      "1281/2000 Loss: 1544.1479797363281, Time:1.33s\n",
      "1282/2000 Loss: 1537.1357421875, Time:1.32s\n",
      "1283/2000 Loss: 1544.109130859375, Time:1.32s\n",
      "1284/2000 Loss: 1537.1160278320312, Time:1.32s\n",
      "1285/2000 Loss: 1544.1305236816406, Time:1.33s\n",
      "1286/2000 Loss: 1537.0960693359375, Time:1.33s\n",
      "1287/2000 Loss: 1544.1889953613281, Time:1.32s\n",
      "1288/2000 Loss: 1537.20068359375, Time:1.32s\n",
      "1289/2000 Loss: 1544.0233764648438, Time:1.31s\n",
      "1290/2000 Loss: 1537.1116638183594, Time:1.32s\n",
      "1291/2000 Loss: 1544.1499633789062, Time:1.32s\n",
      "1292/2000 Loss: 1537.21484375, Time:1.32s\n",
      "1293/2000 Loss: 1544.0396423339844, Time:1.33s\n",
      "1294/2000 Loss: 1537.1279602050781, Time:1.32s\n",
      "1295/2000 Loss: 1544.0706176757812, Time:1.32s\n",
      "1296/2000 Loss: 1537.1069030761719, Time:1.32s\n",
      "1297/2000 Loss: 1544.0527954101562, Time:1.32s\n",
      "1298/2000 Loss: 1537.07763671875, Time:1.33s\n",
      "1299/2000 Loss: 1544.0140380859375, Time:1.32s\n",
      "1300/2000 Loss: 1537.0735473632812, Time:1.32s\n",
      "1301/2000 Loss: 1544.0184631347656, Time:1.31s\n",
      "1302/2000 Loss: 1537.0370483398438, Time:1.33s\n",
      "1303/2000 Loss: 1544.0133056640625, Time:1.31s\n",
      "1304/2000 Loss: 1536.9722595214844, Time:1.32s\n",
      "1305/2000 Loss: 1543.9728698730469, Time:1.32s\n",
      "1306/2000 Loss: 1536.9219970703125, Time:1.32s\n",
      "1307/2000 Loss: 1543.9489135742188, Time:1.32s\n",
      "1308/2000 Loss: 1536.8461303710938, Time:1.31s\n",
      "1309/2000 Loss: 1544.0243225097656, Time:1.33s\n",
      "1310/2000 Loss: 1536.8504028320312, Time:1.33s\n",
      "1311/2000 Loss: 1544.0142211914062, Time:1.32s\n",
      "1312/2000 Loss: 1536.8669128417969, Time:1.33s\n",
      "1313/2000 Loss: 1543.9603271484375, Time:1.31s\n",
      "1314/2000 Loss: 1536.8089294433594, Time:1.34s\n",
      "1315/2000 Loss: 1543.964599609375, Time:1.32s\n",
      "1316/2000 Loss: 1536.8165893554688, Time:1.31s\n",
      "1317/2000 Loss: 1543.9326782226562, Time:1.32s\n",
      "1318/2000 Loss: 1536.7958679199219, Time:1.33s\n",
      "1319/2000 Loss: 1544.017822265625, Time:1.38s\n",
      "1320/2000 Loss: 1536.7982177734375, Time:1.37s\n",
      "1321/2000 Loss: 1543.9244079589844, Time:1.34s\n",
      "1322/2000 Loss: 1536.8367919921875, Time:1.34s\n",
      "1323/2000 Loss: 1543.9114685058594, Time:1.32s\n",
      "1324/2000 Loss: 1536.8382568359375, Time:1.33s\n",
      "1325/2000 Loss: 1543.9717102050781, Time:1.32s\n",
      "1326/2000 Loss: 1536.8408508300781, Time:1.32s\n",
      "1327/2000 Loss: 1543.9513549804688, Time:1.32s\n",
      "1328/2000 Loss: 1536.8543395996094, Time:1.32s\n",
      "1329/2000 Loss: 1543.8394775390625, Time:1.33s\n",
      "1330/2000 Loss: 1536.8146057128906, Time:1.32s\n",
      "1331/2000 Loss: 1543.9443359375, Time:1.3s\n",
      "1332/2000 Loss: 1536.7796630859375, Time:1.32s\n",
      "1333/2000 Loss: 1543.9527893066406, Time:1.34s\n",
      "1334/2000 Loss: 1536.7486267089844, Time:1.32s\n",
      "1335/2000 Loss: 1543.9720458984375, Time:1.31s\n",
      "1336/2000 Loss: 1536.7635192871094, Time:1.33s\n",
      "1337/2000 Loss: 1543.8278503417969, Time:1.32s\n",
      "1338/2000 Loss: 1536.734375, Time:1.32s\n",
      "1339/2000 Loss: 1543.8937377929688, Time:1.32s\n",
      "1340/2000 Loss: 1536.7535400390625, Time:1.31s\n",
      "1341/2000 Loss: 1543.9302978515625, Time:1.32s\n",
      "1342/2000 Loss: 1536.7529602050781, Time:1.33s\n",
      "1343/2000 Loss: 1543.9077758789062, Time:1.31s\n",
      "1344/2000 Loss: 1536.7538757324219, Time:1.33s\n",
      "1345/2000 Loss: 1543.9019775390625, Time:1.32s\n",
      "1346/2000 Loss: 1536.7120361328125, Time:1.32s\n",
      "1347/2000 Loss: 1543.8375244140625, Time:1.31s\n",
      "1348/2000 Loss: 1536.6938171386719, Time:1.33s\n",
      "1349/2000 Loss: 1543.9525756835938, Time:1.32s\n",
      "1350/2000 Loss: 1536.7035217285156, Time:1.32s\n",
      "1351/2000 Loss: 1543.9168090820312, Time:1.33s\n",
      "1352/2000 Loss: 1536.6766662597656, Time:1.32s\n",
      "1353/2000 Loss: 1543.9043884277344, Time:1.35s\n",
      "1354/2000 Loss: 1536.7055358886719, Time:1.32s\n",
      "1355/2000 Loss: 1543.9158325195312, Time:1.32s\n",
      "1356/2000 Loss: 1536.701171875, Time:1.33s\n",
      "1357/2000 Loss: 1543.91064453125, Time:1.32s\n",
      "1358/2000 Loss: 1536.7293701171875, Time:1.34s\n",
      "1359/2000 Loss: 1543.9567260742188, Time:1.32s\n",
      "1360/2000 Loss: 1536.7420654296875, Time:1.32s\n",
      "1361/2000 Loss: 1544.1595458984375, Time:1.42s\n",
      "1362/2000 Loss: 1536.8778991699219, Time:1.43s\n",
      "1363/2000 Loss: 1543.6747436523438, Time:1.42s\n",
      "1364/2000 Loss: 1536.7205505371094, Time:1.38s\n",
      "1365/2000 Loss: 1544.0522155761719, Time:1.47s\n",
      "1366/2000 Loss: 1536.7315673828125, Time:1.32s\n",
      "1367/2000 Loss: 1544.2535095214844, Time:1.32s\n",
      "1368/2000 Loss: 1536.76416015625, Time:1.32s\n",
      "1369/2000 Loss: 1544.0392761230469, Time:1.33s\n",
      "1370/2000 Loss: 1536.7639770507812, Time:1.32s\n",
      "1371/2000 Loss: 1543.8900146484375, Time:1.32s\n",
      "1372/2000 Loss: 1536.7087097167969, Time:1.32s\n",
      "1373/2000 Loss: 1544.1202087402344, Time:1.32s\n",
      "1374/2000 Loss: 1536.6861572265625, Time:1.33s\n",
      "1375/2000 Loss: 1544.251220703125, Time:1.34s\n",
      "1376/2000 Loss: 1536.698974609375, Time:1.32s\n",
      "1377/2000 Loss: 1544.3900756835938, Time:1.32s\n",
      "1378/2000 Loss: 1536.7195129394531, Time:1.36s\n",
      "1379/2000 Loss: 1544.3685302734375, Time:1.38s\n",
      "1380/2000 Loss: 1536.722412109375, Time:1.36s\n",
      "1381/2000 Loss: 1544.1302490234375, Time:1.32s\n",
      "1382/2000 Loss: 1536.7306213378906, Time:1.32s\n",
      "1383/2000 Loss: 1544.0535583496094, Time:1.33s\n",
      "1384/2000 Loss: 1536.7254943847656, Time:1.32s\n",
      "1385/2000 Loss: 1544.344970703125, Time:1.32s\n",
      "1386/2000 Loss: 1536.7591552734375, Time:1.33s\n",
      "1387/2000 Loss: 1544.1095886230469, Time:1.34s\n",
      "1388/2000 Loss: 1536.718994140625, Time:1.33s\n",
      "1389/2000 Loss: 1544.328857421875, Time:1.32s\n",
      "1390/2000 Loss: 1536.6982421875, Time:1.34s\n",
      "1391/2000 Loss: 1544.587646484375, Time:1.33s\n",
      "1392/2000 Loss: 1536.6800842285156, Time:1.32s\n",
      "1393/2000 Loss: 1544.1943359375, Time:1.33s\n",
      "1394/2000 Loss: 1536.6675415039062, Time:1.32s\n",
      "1395/2000 Loss: 1544.5543518066406, Time:1.32s\n",
      "1396/2000 Loss: 1536.7562866210938, Time:1.33s\n",
      "1397/2000 Loss: 1544.82275390625, Time:1.36s\n",
      "1398/2000 Loss: 1536.7487182617188, Time:1.34s\n",
      "1399/2000 Loss: 1544.84375, Time:1.33s\n",
      "1400/2000 Loss: 1536.7018737792969, Time:1.32s\n",
      "1401/2000 Loss: 1544.58642578125, Time:1.33s\n",
      "1402/2000 Loss: 1536.6626281738281, Time:1.33s\n",
      "1403/2000 Loss: 1544.5677795410156, Time:1.32s\n",
      "1404/2000 Loss: 1536.6675415039062, Time:1.32s\n",
      "1405/2000 Loss: 1544.5512084960938, Time:1.33s\n",
      "1406/2000 Loss: 1536.638427734375, Time:1.32s\n",
      "1407/2000 Loss: 1544.2265930175781, Time:1.33s\n",
      "1408/2000 Loss: 1536.6157531738281, Time:1.32s\n",
      "1409/2000 Loss: 1544.5656127929688, Time:1.33s\n",
      "1410/2000 Loss: 1536.6919555664062, Time:1.32s\n",
      "1411/2000 Loss: 1544.6575012207031, Time:1.33s\n",
      "1412/2000 Loss: 1536.6369323730469, Time:1.33s\n",
      "1413/2000 Loss: 1544.4669189453125, Time:1.33s\n",
      "1414/2000 Loss: 1536.6240844726562, Time:1.34s\n",
      "1415/2000 Loss: 1544.4136657714844, Time:1.32s\n",
      "1416/2000 Loss: 1536.6281433105469, Time:1.33s\n",
      "1417/2000 Loss: 1544.5411682128906, Time:1.32s\n",
      "1418/2000 Loss: 1536.6344299316406, Time:1.32s\n",
      "1419/2000 Loss: 1544.2251892089844, Time:1.33s\n",
      "1420/2000 Loss: 1536.5917663574219, Time:1.33s\n",
      "1421/2000 Loss: 1544.5919494628906, Time:1.32s\n",
      "1422/2000 Loss: 1536.6331176757812, Time:1.33s\n",
      "1423/2000 Loss: 1544.4271850585938, Time:1.32s\n",
      "1424/2000 Loss: 1536.58935546875, Time:1.32s\n",
      "1425/2000 Loss: 1544.525146484375, Time:1.32s\n",
      "1426/2000 Loss: 1536.6491394042969, Time:1.32s\n",
      "1427/2000 Loss: 1544.511474609375, Time:1.33s\n",
      "1428/2000 Loss: 1536.5905151367188, Time:1.33s\n",
      "1429/2000 Loss: 1544.3722534179688, Time:1.33s\n",
      "1430/2000 Loss: 1536.5911865234375, Time:1.33s\n",
      "1431/2000 Loss: 1544.1666870117188, Time:1.31s\n",
      "1432/2000 Loss: 1536.6094665527344, Time:1.33s\n",
      "1433/2000 Loss: 1544.3387451171875, Time:1.33s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1434/2000 Loss: 1536.5957946777344, Time:1.32s\n",
      "1435/2000 Loss: 1544.4560546875, Time:1.32s\n",
      "1436/2000 Loss: 1536.5758666992188, Time:1.32s\n",
      "1437/2000 Loss: 1544.3568725585938, Time:1.34s\n",
      "1438/2000 Loss: 1536.6216430664062, Time:1.32s\n",
      "1439/2000 Loss: 1544.2972412109375, Time:1.33s\n",
      "1440/2000 Loss: 1536.6554260253906, Time:1.32s\n",
      "1441/2000 Loss: 1544.3100891113281, Time:1.33s\n",
      "1442/2000 Loss: 1536.5741577148438, Time:1.34s\n",
      "1443/2000 Loss: 1544.134033203125, Time:1.33s\n",
      "1444/2000 Loss: 1536.5751647949219, Time:1.32s\n",
      "1445/2000 Loss: 1544.1622009277344, Time:1.33s\n",
      "1446/2000 Loss: 1536.6049194335938, Time:1.33s\n",
      "1447/2000 Loss: 1544.1065368652344, Time:1.34s\n",
      "1448/2000 Loss: 1536.5972900390625, Time:1.33s\n",
      "1449/2000 Loss: 1544.1208190917969, Time:1.33s\n",
      "1450/2000 Loss: 1536.5856018066406, Time:1.34s\n",
      "1451/2000 Loss: 1544.0648498535156, Time:1.33s\n",
      "1452/2000 Loss: 1536.5648803710938, Time:1.33s\n",
      "1453/2000 Loss: 1544.0605163574219, Time:1.33s\n",
      "1454/2000 Loss: 1536.57958984375, Time:1.33s\n",
      "1455/2000 Loss: 1544.091552734375, Time:1.33s\n",
      "1456/2000 Loss: 1536.6436462402344, Time:1.34s\n",
      "1457/2000 Loss: 1544.176513671875, Time:1.34s\n",
      "1458/2000 Loss: 1536.6012268066406, Time:1.33s\n",
      "1459/2000 Loss: 1544.4494323730469, Time:1.33s\n",
      "1460/2000 Loss: 1536.6054992675781, Time:1.34s\n",
      "1461/2000 Loss: 1544.2608947753906, Time:1.32s\n",
      "1462/2000 Loss: 1536.5821838378906, Time:1.33s\n",
      "1463/2000 Loss: 1544.2940673828125, Time:1.33s\n",
      "1464/2000 Loss: 1536.5878295898438, Time:1.34s\n",
      "1465/2000 Loss: 1544.4791564941406, Time:1.34s\n",
      "1466/2000 Loss: 1536.6614685058594, Time:1.33s\n",
      "1467/2000 Loss: 1544.1633605957031, Time:1.32s\n",
      "1468/2000 Loss: 1536.6368713378906, Time:1.35s\n",
      "1469/2000 Loss: 1544.0186462402344, Time:1.34s\n",
      "1470/2000 Loss: 1536.6277770996094, Time:1.32s\n",
      "1471/2000 Loss: 1544.1459045410156, Time:1.33s\n",
      "1472/2000 Loss: 1536.6356506347656, Time:1.34s\n",
      "1473/2000 Loss: 1544.1273498535156, Time:1.32s\n",
      "1474/2000 Loss: 1536.5989685058594, Time:1.33s\n",
      "1475/2000 Loss: 1544.0203247070312, Time:1.32s\n",
      "1476/2000 Loss: 1536.5545654296875, Time:1.31s\n",
      "1477/2000 Loss: 1544.0539245605469, Time:1.32s\n",
      "1478/2000 Loss: 1536.5610046386719, Time:1.32s\n",
      "1479/2000 Loss: 1543.9717712402344, Time:1.32s\n",
      "1480/2000 Loss: 1536.5316772460938, Time:1.33s\n",
      "1481/2000 Loss: 1543.9410095214844, Time:1.34s\n",
      "1482/2000 Loss: 1536.5020141601562, Time:1.33s\n",
      "1483/2000 Loss: 1544.0848693847656, Time:1.5s\n",
      "1484/2000 Loss: 1536.5398559570312, Time:1.4s\n",
      "1485/2000 Loss: 1544.1742858886719, Time:1.32s\n",
      "1486/2000 Loss: 1536.5503540039062, Time:1.32s\n",
      "1487/2000 Loss: 1544.2513122558594, Time:1.33s\n",
      "1488/2000 Loss: 1536.5527038574219, Time:1.33s\n",
      "1489/2000 Loss: 1544.2351684570312, Time:1.32s\n",
      "1490/2000 Loss: 1536.5632629394531, Time:1.33s\n",
      "1491/2000 Loss: 1544.1823120117188, Time:1.32s\n",
      "1492/2000 Loss: 1536.5305480957031, Time:1.34s\n",
      "1493/2000 Loss: 1544.0978698730469, Time:1.32s\n",
      "1494/2000 Loss: 1536.55908203125, Time:1.32s\n",
      "1495/2000 Loss: 1544.0795288085938, Time:1.33s\n",
      "1496/2000 Loss: 1536.5240173339844, Time:1.32s\n",
      "1497/2000 Loss: 1543.9579467773438, Time:1.31s\n",
      "1498/2000 Loss: 1536.4981994628906, Time:1.33s\n",
      "1499/2000 Loss: 1543.9855651855469, Time:1.32s\n",
      "1500/2000 Loss: 1536.519287109375, Time:1.32s\n",
      "1501/2000 Loss: 1544.0972900390625, Time:1.32s\n",
      "1502/2000 Loss: 1536.5505065917969, Time:1.32s\n",
      "1503/2000 Loss: 1543.9693603515625, Time:1.33s\n",
      "1504/2000 Loss: 1536.6116027832031, Time:1.32s\n",
      "1505/2000 Loss: 1544.0186767578125, Time:1.33s\n",
      "1506/2000 Loss: 1536.5697937011719, Time:1.33s\n",
      "1507/2000 Loss: 1543.8644104003906, Time:1.33s\n",
      "1508/2000 Loss: 1536.5255737304688, Time:1.32s\n",
      "1509/2000 Loss: 1543.927978515625, Time:1.31s\n",
      "1510/2000 Loss: 1536.5513916015625, Time:1.32s\n",
      "1511/2000 Loss: 1543.8180541992188, Time:1.33s\n",
      "1512/2000 Loss: 1536.5354919433594, Time:1.32s\n",
      "1513/2000 Loss: 1544.0422973632812, Time:1.32s\n",
      "1514/2000 Loss: 1536.5198974609375, Time:1.33s\n",
      "1515/2000 Loss: 1543.9910888671875, Time:1.32s\n",
      "1516/2000 Loss: 1536.5414123535156, Time:1.33s\n",
      "1517/2000 Loss: 1544.0593872070312, Time:1.32s\n",
      "1518/2000 Loss: 1536.5196838378906, Time:1.33s\n",
      "1519/2000 Loss: 1544.1236267089844, Time:1.33s\n",
      "1520/2000 Loss: 1536.5209655761719, Time:1.33s\n",
      "1521/2000 Loss: 1544.097900390625, Time:1.32s\n",
      "1522/2000 Loss: 1536.5744018554688, Time:1.33s\n",
      "1523/2000 Loss: 1544.1884460449219, Time:1.33s\n",
      "1524/2000 Loss: 1536.6015319824219, Time:1.32s\n",
      "1525/2000 Loss: 1544.3697509765625, Time:1.34s\n",
      "1526/2000 Loss: 1536.56201171875, Time:1.34s\n",
      "1527/2000 Loss: 1544.0261535644531, Time:1.33s\n",
      "1528/2000 Loss: 1536.5435485839844, Time:1.32s\n",
      "1529/2000 Loss: 1544.39208984375, Time:1.33s\n",
      "1530/2000 Loss: 1536.5458679199219, Time:1.34s\n",
      "1531/2000 Loss: 1544.1938781738281, Time:1.32s\n",
      "1532/2000 Loss: 1536.5057983398438, Time:1.32s\n",
      "1533/2000 Loss: 1544.2754211425781, Time:1.33s\n",
      "1534/2000 Loss: 1536.5687255859375, Time:1.32s\n",
      "1535/2000 Loss: 1544.2621765136719, Time:1.33s\n",
      "1536/2000 Loss: 1536.5230102539062, Time:1.32s\n",
      "1537/2000 Loss: 1544.0912170410156, Time:1.33s\n",
      "1538/2000 Loss: 1536.5415649414062, Time:1.33s\n",
      "1539/2000 Loss: 1544.3929443359375, Time:1.32s\n",
      "1540/2000 Loss: 1536.58837890625, Time:1.34s\n",
      "1541/2000 Loss: 1544.1884155273438, Time:1.33s\n",
      "1542/2000 Loss: 1536.5616455078125, Time:1.34s\n",
      "1543/2000 Loss: 1543.7462768554688, Time:1.33s\n",
      "1544/2000 Loss: 1536.5322875976562, Time:1.32s\n",
      "1545/2000 Loss: 1544.0013427734375, Time:1.32s\n",
      "1546/2000 Loss: 1536.5492858886719, Time:1.32s\n",
      "1547/2000 Loss: 1544.3390808105469, Time:1.32s\n",
      "1548/2000 Loss: 1536.5394897460938, Time:1.32s\n",
      "1549/2000 Loss: 1544.1141052246094, Time:1.32s\n",
      "1550/2000 Loss: 1536.6063842773438, Time:1.33s\n",
      "1551/2000 Loss: 1543.9998168945312, Time:1.33s\n",
      "1552/2000 Loss: 1536.5508728027344, Time:1.32s\n",
      "1553/2000 Loss: 1543.7618408203125, Time:1.34s\n",
      "1554/2000 Loss: 1536.534423828125, Time:1.32s\n",
      "1555/2000 Loss: 1544.1490478515625, Time:1.32s\n",
      "1556/2000 Loss: 1536.5874938964844, Time:1.43s\n",
      "1557/2000 Loss: 1544.1768188476562, Time:1.45s\n",
      "1558/2000 Loss: 1536.5725708007812, Time:1.47s\n",
      "1559/2000 Loss: 1544.0479736328125, Time:1.39s\n",
      "1560/2000 Loss: 1536.5194396972656, Time:1.36s\n",
      "1561/2000 Loss: 1544.115966796875, Time:1.39s\n",
      "1562/2000 Loss: 1536.5265197753906, Time:1.39s\n",
      "1563/2000 Loss: 1544.0772094726562, Time:1.39s\n",
      "1564/2000 Loss: 1536.47900390625, Time:1.36s\n",
      "1565/2000 Loss: 1544.3816223144531, Time:1.48s\n",
      "1566/2000 Loss: 1536.51904296875, Time:1.64s\n",
      "1567/2000 Loss: 1544.0332641601562, Time:1.4s\n",
      "1568/2000 Loss: 1536.4775390625, Time:1.39s\n",
      "1569/2000 Loss: 1543.9422912597656, Time:1.45s\n",
      "1570/2000 Loss: 1536.5298767089844, Time:1.4s\n",
      "1571/2000 Loss: 1544.2663269042969, Time:1.42s\n",
      "1572/2000 Loss: 1536.57958984375, Time:1.92s\n",
      "1573/2000 Loss: 1544.1477661132812, Time:1.43s\n",
      "1574/2000 Loss: 1536.4900817871094, Time:1.32s\n",
      "1575/2000 Loss: 1543.8958740234375, Time:1.32s\n",
      "1576/2000 Loss: 1536.4544677734375, Time:1.33s\n",
      "1577/2000 Loss: 1543.9501647949219, Time:1.41s\n",
      "1578/2000 Loss: 1536.5062255859375, Time:1.33s\n",
      "1579/2000 Loss: 1544.1372375488281, Time:1.39s\n",
      "1580/2000 Loss: 1536.531494140625, Time:1.37s\n",
      "1581/2000 Loss: 1544.077392578125, Time:1.38s\n",
      "1582/2000 Loss: 1536.4790649414062, Time:1.39s\n",
      "1583/2000 Loss: 1544.051513671875, Time:1.39s\n",
      "1584/2000 Loss: 1536.474853515625, Time:1.37s\n",
      "1585/2000 Loss: 1543.8878784179688, Time:1.37s\n",
      "1586/2000 Loss: 1536.43798828125, Time:1.37s\n",
      "1587/2000 Loss: 1543.978515625, Time:1.38s\n",
      "1588/2000 Loss: 1536.4855346679688, Time:1.32s\n",
      "1589/2000 Loss: 1544.0320434570312, Time:1.31s\n",
      "1590/2000 Loss: 1536.4553527832031, Time:1.31s\n",
      "1591/2000 Loss: 1543.9214172363281, Time:1.31s\n",
      "1592/2000 Loss: 1536.4148254394531, Time:1.34s\n",
      "1593/2000 Loss: 1543.9306030273438, Time:1.48s\n",
      "1594/2000 Loss: 1536.4382019042969, Time:1.43s\n",
      "1595/2000 Loss: 1543.9967956542969, Time:1.37s\n",
      "1596/2000 Loss: 1536.4617004394531, Time:1.56s\n",
      "1597/2000 Loss: 1543.9710998535156, Time:1.34s\n",
      "1598/2000 Loss: 1536.4787902832031, Time:1.37s\n",
      "1599/2000 Loss: 1544.1404418945312, Time:1.37s\n",
      "1600/2000 Loss: 1536.4877624511719, Time:1.32s\n",
      "1601/2000 Loss: 1543.9731140136719, Time:1.35s\n",
      "1602/2000 Loss: 1536.4337158203125, Time:1.33s\n",
      "1603/2000 Loss: 1543.9978332519531, Time:1.33s\n",
      "1604/2000 Loss: 1536.4752502441406, Time:1.35s\n",
      "1605/2000 Loss: 1544.2128601074219, Time:1.34s\n",
      "1606/2000 Loss: 1536.4531555175781, Time:1.32s\n",
      "1607/2000 Loss: 1544.3257446289062, Time:1.34s\n",
      "1608/2000 Loss: 1536.4835205078125, Time:1.34s\n",
      "1609/2000 Loss: 1544.1236572265625, Time:1.33s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1610/2000 Loss: 1536.4918518066406, Time:1.41s\n",
      "1611/2000 Loss: 1544.2947998046875, Time:1.38s\n",
      "1612/2000 Loss: 1536.5606994628906, Time:1.36s\n",
      "1613/2000 Loss: 1544.3135681152344, Time:1.36s\n",
      "1614/2000 Loss: 1536.4688110351562, Time:1.35s\n",
      "1615/2000 Loss: 1544.17236328125, Time:1.35s\n",
      "1616/2000 Loss: 1536.4858703613281, Time:1.34s\n",
      "1617/2000 Loss: 1544.3015441894531, Time:1.32s\n",
      "1618/2000 Loss: 1536.4978332519531, Time:1.33s\n",
      "1619/2000 Loss: 1544.4043884277344, Time:1.33s\n",
      "1620/2000 Loss: 1536.5411987304688, Time:1.33s\n",
      "1621/2000 Loss: 1544.2738037109375, Time:1.34s\n",
      "1622/2000 Loss: 1536.4885864257812, Time:1.34s\n",
      "1623/2000 Loss: 1544.6088256835938, Time:1.34s\n",
      "1624/2000 Loss: 1536.6687316894531, Time:1.33s\n",
      "1625/2000 Loss: 1544.2016906738281, Time:1.34s\n",
      "1626/2000 Loss: 1536.4759216308594, Time:1.33s\n",
      "1627/2000 Loss: 1544.9504089355469, Time:1.34s\n",
      "1628/2000 Loss: 1536.5414123535156, Time:1.33s\n",
      "1629/2000 Loss: 1544.5403442382812, Time:1.33s\n",
      "1630/2000 Loss: 1536.5027160644531, Time:1.33s\n",
      "1631/2000 Loss: 1543.9834899902344, Time:1.32s\n",
      "1632/2000 Loss: 1536.4904479980469, Time:1.41s\n",
      "1633/2000 Loss: 1544.3818054199219, Time:1.35s\n",
      "1634/2000 Loss: 1536.4567260742188, Time:1.33s\n",
      "1635/2000 Loss: 1544.3001098632812, Time:1.33s\n",
      "1636/2000 Loss: 1536.4969482421875, Time:1.34s\n",
      "1637/2000 Loss: 1544.5484008789062, Time:1.32s\n",
      "1638/2000 Loss: 1536.4414978027344, Time:1.32s\n",
      "1639/2000 Loss: 1544.7708129882812, Time:1.34s\n",
      "1640/2000 Loss: 1536.6144714355469, Time:1.33s\n",
      "1641/2000 Loss: 1544.6061096191406, Time:1.33s\n",
      "1642/2000 Loss: 1536.5156860351562, Time:1.33s\n",
      "1643/2000 Loss: 1545.6121215820312, Time:1.33s\n",
      "1644/2000 Loss: 1536.6963500976562, Time:1.32s\n",
      "1645/2000 Loss: 1545.1384582519531, Time:1.33s\n",
      "1646/2000 Loss: 1536.5265502929688, Time:1.32s\n",
      "1647/2000 Loss: 1544.7204284667969, Time:1.33s\n",
      "1648/2000 Loss: 1536.5064086914062, Time:1.33s\n",
      "1649/2000 Loss: 1545.0851135253906, Time:1.34s\n",
      "1650/2000 Loss: 1536.5553894042969, Time:1.33s\n",
      "1651/2000 Loss: 1544.8668212890625, Time:1.33s\n",
      "1652/2000 Loss: 1536.5688781738281, Time:1.33s\n",
      "1653/2000 Loss: 1545.230224609375, Time:1.34s\n",
      "1654/2000 Loss: 1536.6881103515625, Time:1.33s\n",
      "1655/2000 Loss: 1545.1164855957031, Time:1.32s\n",
      "1656/2000 Loss: 1536.5535278320312, Time:1.32s\n",
      "1657/2000 Loss: 1545.1485290527344, Time:1.33s\n",
      "1658/2000 Loss: 1536.6546020507812, Time:1.33s\n",
      "1659/2000 Loss: 1545.3160705566406, Time:1.33s\n",
      "1660/2000 Loss: 1536.6278991699219, Time:1.34s\n",
      "1661/2000 Loss: 1545.6727905273438, Time:1.34s\n",
      "1662/2000 Loss: 1536.7512512207031, Time:1.33s\n",
      "1663/2000 Loss: 1544.8928833007812, Time:1.32s\n",
      "1664/2000 Loss: 1536.60498046875, Time:1.34s\n",
      "1665/2000 Loss: 1546.0477905273438, Time:1.33s\n",
      "1666/2000 Loss: 1536.8298645019531, Time:1.34s\n",
      "1667/2000 Loss: 1545.57861328125, Time:1.32s\n",
      "1668/2000 Loss: 1536.6893615722656, Time:1.35s\n",
      "1669/2000 Loss: 1545.2698059082031, Time:1.33s\n",
      "1670/2000 Loss: 1536.6400146484375, Time:1.32s\n",
      "1671/2000 Loss: 1545.2855224609375, Time:1.34s\n",
      "1672/2000 Loss: 1536.6047973632812, Time:1.33s\n",
      "1673/2000 Loss: 1545.5042419433594, Time:1.32s\n",
      "1674/2000 Loss: 1536.7393188476562, Time:1.36s\n",
      "1675/2000 Loss: 1545.5917663574219, Time:1.47s\n",
      "1676/2000 Loss: 1536.6492614746094, Time:1.41s\n",
      "1677/2000 Loss: 1545.1792602539062, Time:1.45s\n",
      "1678/2000 Loss: 1536.6699523925781, Time:1.38s\n",
      "1679/2000 Loss: 1545.251708984375, Time:1.33s\n",
      "1680/2000 Loss: 1536.6152038574219, Time:1.33s\n",
      "1681/2000 Loss: 1544.9944458007812, Time:1.32s\n",
      "1682/2000 Loss: 1536.6085815429688, Time:1.37s\n",
      "1683/2000 Loss: 1545.4584045410156, Time:1.43s\n",
      "1684/2000 Loss: 1536.5774230957031, Time:1.32s\n",
      "1685/2000 Loss: 1545.0960083007812, Time:1.32s\n",
      "1686/2000 Loss: 1536.5966796875, Time:1.31s\n",
      "1687/2000 Loss: 1545.6604309082031, Time:1.32s\n",
      "1688/2000 Loss: 1536.7345581054688, Time:1.32s\n",
      "1689/2000 Loss: 1545.3934631347656, Time:1.44s\n",
      "1690/2000 Loss: 1536.7150268554688, Time:1.5s\n",
      "1691/2000 Loss: 1545.8531799316406, Time:1.37s\n",
      "1692/2000 Loss: 1536.7307434082031, Time:1.35s\n",
      "1693/2000 Loss: 1545.3854370117188, Time:1.39s\n",
      "1694/2000 Loss: 1536.5956115722656, Time:1.45s\n",
      "1695/2000 Loss: 1545.3249206542969, Time:1.39s\n",
      "1696/2000 Loss: 1536.6394348144531, Time:1.4s\n",
      "1697/2000 Loss: 1545.3977661132812, Time:1.31s\n",
      "1698/2000 Loss: 1536.6839904785156, Time:1.37s\n",
      "1699/2000 Loss: 1545.4752807617188, Time:1.35s\n",
      "1700/2000 Loss: 1536.6810913085938, Time:1.36s\n",
      "1701/2000 Loss: 1545.2967834472656, Time:1.34s\n",
      "1702/2000 Loss: 1536.6964111328125, Time:1.4s\n",
      "1703/2000 Loss: 1545.1585388183594, Time:1.39s\n",
      "1704/2000 Loss: 1536.6873779296875, Time:1.43s\n",
      "1705/2000 Loss: 1545.2428588867188, Time:1.43s\n",
      "1706/2000 Loss: 1536.7199096679688, Time:1.48s\n",
      "1707/2000 Loss: 1545.0818786621094, Time:1.57s\n",
      "1708/2000 Loss: 1536.8973999023438, Time:1.41s\n",
      "1709/2000 Loss: 1546.2740783691406, Time:1.41s\n",
      "1710/2000 Loss: 1536.7922668457031, Time:1.33s\n",
      "1711/2000 Loss: 1545.1459655761719, Time:1.34s\n",
      "1712/2000 Loss: 1536.751220703125, Time:1.36s\n",
      "1713/2000 Loss: 1545.7712707519531, Time:1.34s\n",
      "1714/2000 Loss: 1536.7550048828125, Time:1.35s\n",
      "1715/2000 Loss: 1545.5962524414062, Time:1.33s\n",
      "1716/2000 Loss: 1536.7033386230469, Time:1.44s\n",
      "1717/2000 Loss: 1546.0185546875, Time:1.53s\n",
      "1718/2000 Loss: 1536.7109375, Time:1.45s\n",
      "1719/2000 Loss: 1545.5250854492188, Time:1.49s\n",
      "1720/2000 Loss: 1536.7098999023438, Time:1.34s\n",
      "1721/2000 Loss: 1545.4093017578125, Time:1.36s\n",
      "1722/2000 Loss: 1536.7068176269531, Time:1.52s\n",
      "1723/2000 Loss: 1546.02392578125, Time:1.4s\n",
      "1724/2000 Loss: 1536.7943725585938, Time:1.43s\n",
      "1725/2000 Loss: 1545.2273559570312, Time:1.36s\n",
      "1726/2000 Loss: 1536.6668395996094, Time:1.36s\n",
      "1727/2000 Loss: 1545.5683898925781, Time:1.35s\n",
      "1728/2000 Loss: 1536.7795104980469, Time:1.36s\n",
      "1729/2000 Loss: 1545.9757690429688, Time:1.54s\n",
      "1730/2000 Loss: 1536.7887878417969, Time:1.45s\n",
      "1731/2000 Loss: 1545.4337768554688, Time:1.36s\n",
      "1732/2000 Loss: 1536.6561889648438, Time:1.36s\n",
      "1733/2000 Loss: 1545.5064086914062, Time:1.35s\n",
      "1734/2000 Loss: 1536.6802978515625, Time:1.36s\n",
      "1735/2000 Loss: 1545.5164794921875, Time:1.35s\n",
      "1736/2000 Loss: 1536.6356811523438, Time:1.37s\n",
      "1737/2000 Loss: 1545.7299194335938, Time:1.36s\n",
      "1738/2000 Loss: 1536.6643371582031, Time:1.35s\n",
      "1739/2000 Loss: 1545.703369140625, Time:1.38s\n",
      "1740/2000 Loss: 1536.7225036621094, Time:1.36s\n",
      "1741/2000 Loss: 1545.7318420410156, Time:1.44s\n",
      "1742/2000 Loss: 1536.6957702636719, Time:1.52s\n",
      "1743/2000 Loss: 1545.9094848632812, Time:1.58s\n",
      "1744/2000 Loss: 1536.7105102539062, Time:1.54s\n",
      "1745/2000 Loss: 1545.5417785644531, Time:1.43s\n",
      "1746/2000 Loss: 1536.6681213378906, Time:1.64s\n",
      "1747/2000 Loss: 1545.9703979492188, Time:1.32s\n",
      "1748/2000 Loss: 1536.6297302246094, Time:1.33s\n",
      "1749/2000 Loss: 1545.6816101074219, Time:1.34s\n",
      "1750/2000 Loss: 1536.6192016601562, Time:1.46s\n",
      "1751/2000 Loss: 1545.9424133300781, Time:1.48s\n",
      "1752/2000 Loss: 1536.7637939453125, Time:1.36s\n",
      "1753/2000 Loss: 1545.81201171875, Time:1.33s\n",
      "1754/2000 Loss: 1537.0194091796875, Time:1.32s\n",
      "1755/2000 Loss: 1545.8336486816406, Time:1.33s\n",
      "1756/2000 Loss: 1536.7919311523438, Time:1.32s\n",
      "1757/2000 Loss: 1545.7075805664062, Time:1.36s\n",
      "1758/2000 Loss: 1536.8116149902344, Time:1.34s\n",
      "1759/2000 Loss: 1545.7539978027344, Time:1.4s\n",
      "1760/2000 Loss: 1536.8918151855469, Time:1.65s\n",
      "1761/2000 Loss: 1545.3019714355469, Time:1.58s\n",
      "1762/2000 Loss: 1536.7893981933594, Time:1.61s\n",
      "1763/2000 Loss: 1545.4751586914062, Time:1.46s\n",
      "1764/2000 Loss: 1536.8724060058594, Time:1.37s\n",
      "1765/2000 Loss: 1545.1466979980469, Time:1.44s\n",
      "1766/2000 Loss: 1536.87646484375, Time:1.4s\n",
      "1767/2000 Loss: 1545.15869140625, Time:1.46s\n",
      "1768/2000 Loss: 1536.7525939941406, Time:1.32s\n",
      "1769/2000 Loss: 1545.2432250976562, Time:1.35s\n",
      "1770/2000 Loss: 1536.78466796875, Time:1.34s\n",
      "1771/2000 Loss: 1545.1917419433594, Time:1.32s\n",
      "1772/2000 Loss: 1536.8445434570312, Time:1.32s\n",
      "1773/2000 Loss: 1545.3018188476562, Time:1.34s\n",
      "1774/2000 Loss: 1536.8072814941406, Time:1.37s\n",
      "1775/2000 Loss: 1545.3213500976562, Time:1.54s\n",
      "1776/2000 Loss: 1536.8115539550781, Time:1.35s\n",
      "1777/2000 Loss: 1545.1512756347656, Time:1.37s\n",
      "1778/2000 Loss: 1536.7990417480469, Time:1.36s\n",
      "1779/2000 Loss: 1545.1173706054688, Time:1.43s\n",
      "1780/2000 Loss: 1536.7449645996094, Time:1.32s\n",
      "1781/2000 Loss: 1545.1210327148438, Time:1.34s\n",
      "1782/2000 Loss: 1536.810302734375, Time:1.43s\n",
      "1783/2000 Loss: 1545.2615966796875, Time:1.34s\n",
      "1784/2000 Loss: 1536.7901306152344, Time:1.38s\n",
      "1785/2000 Loss: 1545.192138671875, Time:1.37s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1786/2000 Loss: 1536.7975769042969, Time:1.34s\n",
      "1787/2000 Loss: 1545.2658081054688, Time:1.32s\n",
      "1788/2000 Loss: 1536.724853515625, Time:1.38s\n",
      "1789/2000 Loss: 1545.1149597167969, Time:1.35s\n",
      "1790/2000 Loss: 1536.7951354980469, Time:1.32s\n",
      "1791/2000 Loss: 1545.19873046875, Time:1.31s\n",
      "1792/2000 Loss: 1536.8395690917969, Time:1.38s\n",
      "1793/2000 Loss: 1545.1441345214844, Time:1.43s\n",
      "1794/2000 Loss: 1536.7889709472656, Time:1.34s\n",
      "1795/2000 Loss: 1545.083984375, Time:1.47s\n",
      "1796/2000 Loss: 1536.818603515625, Time:1.44s\n",
      "1797/2000 Loss: 1545.044921875, Time:1.41s\n",
      "1798/2000 Loss: 1536.7647094726562, Time:1.41s\n",
      "1799/2000 Loss: 1545.063720703125, Time:1.33s\n",
      "1800/2000 Loss: 1536.8066711425781, Time:1.38s\n",
      "1801/2000 Loss: 1545.0275268554688, Time:1.33s\n",
      "1802/2000 Loss: 1536.7983703613281, Time:1.36s\n",
      "1803/2000 Loss: 1545.0885009765625, Time:1.43s\n",
      "1804/2000 Loss: 1536.7601013183594, Time:1.37s\n",
      "1805/2000 Loss: 1545.0996398925781, Time:1.33s\n",
      "1806/2000 Loss: 1536.8279418945312, Time:1.62s\n",
      "1807/2000 Loss: 1545.0128173828125, Time:1.91s\n",
      "1808/2000 Loss: 1536.7983703613281, Time:1.29s\n",
      "1809/2000 Loss: 1545.3896179199219, Time:1.41s\n",
      "1810/2000 Loss: 1536.8265991210938, Time:1.33s\n",
      "1811/2000 Loss: 1544.75341796875, Time:1.39s\n",
      "1812/2000 Loss: 1536.7545471191406, Time:1.36s\n",
      "1813/2000 Loss: 1545.0822143554688, Time:1.35s\n",
      "1814/2000 Loss: 1536.7587585449219, Time:1.35s\n",
      "1815/2000 Loss: 1545.2014770507812, Time:1.29s\n",
      "1816/2000 Loss: 1536.8827209472656, Time:1.29s\n",
      "1817/2000 Loss: 1544.9334411621094, Time:1.3s\n",
      "1818/2000 Loss: 1536.8537292480469, Time:1.4s\n",
      "1819/2000 Loss: 1544.7823181152344, Time:1.44s\n",
      "1820/2000 Loss: 1536.7625427246094, Time:1.37s\n",
      "1821/2000 Loss: 1544.9940795898438, Time:1.43s\n",
      "1822/2000 Loss: 1536.8368225097656, Time:1.46s\n",
      "1823/2000 Loss: 1545.0693969726562, Time:1.45s\n",
      "1824/2000 Loss: 1536.8367614746094, Time:1.65s\n",
      "1825/2000 Loss: 1544.8710327148438, Time:1.46s\n",
      "1826/2000 Loss: 1536.877197265625, Time:1.41s\n",
      "1827/2000 Loss: 1544.9260559082031, Time:1.46s\n",
      "1828/2000 Loss: 1536.8526306152344, Time:1.44s\n",
      "1829/2000 Loss: 1545.1559143066406, Time:1.35s\n",
      "1830/2000 Loss: 1536.8164367675781, Time:1.43s\n",
      "1831/2000 Loss: 1544.8928833007812, Time:1.4s\n",
      "1832/2000 Loss: 1536.8516540527344, Time:1.32s\n",
      "1833/2000 Loss: 1544.8885192871094, Time:1.31s\n",
      "1834/2000 Loss: 1536.8566284179688, Time:1.29s\n",
      "1835/2000 Loss: 1544.9394226074219, Time:1.33s\n",
      "1836/2000 Loss: 1536.8738708496094, Time:1.45s\n",
      "1837/2000 Loss: 1545.0510559082031, Time:1.29s\n",
      "1838/2000 Loss: 1536.8821411132812, Time:1.38s\n",
      "1839/2000 Loss: 1544.7384033203125, Time:1.45s\n",
      "1840/2000 Loss: 1536.8876953125, Time:1.59s\n",
      "1841/2000 Loss: 1544.7711181640625, Time:1.39s\n",
      "1842/2000 Loss: 1536.8187866210938, Time:1.39s\n",
      "1843/2000 Loss: 1544.814208984375, Time:1.32s\n",
      "1844/2000 Loss: 1536.8547973632812, Time:1.33s\n",
      "1845/2000 Loss: 1544.9069519042969, Time:1.34s\n",
      "1846/2000 Loss: 1536.8845520019531, Time:1.38s\n",
      "1847/2000 Loss: 1544.6834411621094, Time:1.34s\n",
      "1848/2000 Loss: 1536.8444213867188, Time:1.39s\n",
      "1849/2000 Loss: 1544.9124450683594, Time:1.34s\n",
      "1850/2000 Loss: 1536.9274291992188, Time:1.38s\n",
      "1851/2000 Loss: 1544.6919250488281, Time:1.38s\n",
      "1852/2000 Loss: 1536.8707580566406, Time:1.38s\n",
      "1853/2000 Loss: 1544.7469787597656, Time:1.36s\n",
      "1854/2000 Loss: 1536.8270263671875, Time:1.3s\n",
      "1855/2000 Loss: 1544.6029052734375, Time:1.34s\n",
      "1856/2000 Loss: 1536.8452453613281, Time:1.34s\n",
      "1857/2000 Loss: 1544.6345520019531, Time:1.38s\n",
      "1858/2000 Loss: 1536.8211975097656, Time:1.33s\n",
      "1859/2000 Loss: 1544.6659851074219, Time:1.33s\n",
      "1860/2000 Loss: 1536.8179321289062, Time:1.35s\n",
      "1861/2000 Loss: 1544.8588562011719, Time:1.32s\n",
      "1862/2000 Loss: 1536.8767395019531, Time:1.31s\n",
      "1863/2000 Loss: 1544.4793701171875, Time:1.38s\n",
      "1864/2000 Loss: 1536.7919921875, Time:1.54s\n",
      "1865/2000 Loss: 1544.6249694824219, Time:1.29s\n",
      "1866/2000 Loss: 1536.839599609375, Time:1.3s\n",
      "1867/2000 Loss: 1544.6891479492188, Time:1.55s\n",
      "1868/2000 Loss: 1536.8187561035156, Time:1.29s\n",
      "1869/2000 Loss: 1544.7066040039062, Time:1.3s\n",
      "1870/2000 Loss: 1536.9096374511719, Time:1.3s\n",
      "1871/2000 Loss: 1544.5658569335938, Time:1.29s\n",
      "1872/2000 Loss: 1536.7914428710938, Time:1.3s\n",
      "1873/2000 Loss: 1544.8226318359375, Time:1.31s\n",
      "1874/2000 Loss: 1536.7825317382812, Time:1.29s\n",
      "1875/2000 Loss: 1544.4681701660156, Time:1.41s\n",
      "1876/2000 Loss: 1536.7296142578125, Time:1.36s\n",
      "1877/2000 Loss: 1544.4559631347656, Time:1.3s\n",
      "1878/2000 Loss: 1536.8073425292969, Time:1.47s\n",
      "1879/2000 Loss: 1544.5618286132812, Time:1.44s\n",
      "1880/2000 Loss: 1536.8502502441406, Time:1.33s\n",
      "1881/2000 Loss: 1544.9822998046875, Time:1.3s\n",
      "1882/2000 Loss: 1536.89208984375, Time:1.3s\n",
      "1883/2000 Loss: 1545.2026672363281, Time:1.31s\n",
      "1884/2000 Loss: 1536.8560180664062, Time:1.58s\n",
      "1885/2000 Loss: 1545.0717468261719, Time:1.58s\n",
      "1886/2000 Loss: 1536.748779296875, Time:1.37s\n",
      "1887/2000 Loss: 1545.0484313964844, Time:1.32s\n",
      "1888/2000 Loss: 1536.7530517578125, Time:1.4s\n",
      "1889/2000 Loss: 1545.110595703125, Time:1.41s\n",
      "1890/2000 Loss: 1536.8003845214844, Time:1.48s\n",
      "1891/2000 Loss: 1545.2248229980469, Time:1.3s\n",
      "1892/2000 Loss: 1536.708984375, Time:1.3s\n",
      "1893/2000 Loss: 1545.2288208007812, Time:1.3s\n",
      "1894/2000 Loss: 1536.6771545410156, Time:1.31s\n",
      "1895/2000 Loss: 1545.1139221191406, Time:1.45s\n",
      "1896/2000 Loss: 1536.7581481933594, Time:1.3s\n",
      "1897/2000 Loss: 1545.0726623535156, Time:1.3s\n",
      "1898/2000 Loss: 1536.7123107910156, Time:1.31s\n",
      "1899/2000 Loss: 1545.1268310546875, Time:1.3s\n",
      "1900/2000 Loss: 1536.6978149414062, Time:1.41s\n",
      "1901/2000 Loss: 1545.4146423339844, Time:1.61s\n",
      "1902/2000 Loss: 1536.7713928222656, Time:1.37s\n",
      "1903/2000 Loss: 1545.2178649902344, Time:1.3s\n",
      "1904/2000 Loss: 1536.7609558105469, Time:1.29s\n",
      "1905/2000 Loss: 1545.1836853027344, Time:1.42s\n",
      "1906/2000 Loss: 1536.70458984375, Time:1.3s\n",
      "1907/2000 Loss: 1545.2094421386719, Time:1.31s\n",
      "1908/2000 Loss: 1536.7129516601562, Time:1.3s\n",
      "1909/2000 Loss: 1545.3580017089844, Time:1.31s\n",
      "1910/2000 Loss: 1536.7843627929688, Time:1.5s\n",
      "1911/2000 Loss: 1545.0867004394531, Time:1.5s\n",
      "1912/2000 Loss: 1536.7988586425781, Time:1.3s\n",
      "1913/2000 Loss: 1545.6011962890625, Time:1.31s\n",
      "1914/2000 Loss: 1536.78271484375, Time:1.31s\n",
      "1915/2000 Loss: 1545.61279296875, Time:1.45s\n",
      "1916/2000 Loss: 1536.8121948242188, Time:1.31s\n",
      "1917/2000 Loss: 1545.3399963378906, Time:1.41s\n",
      "1918/2000 Loss: 1536.8579711914062, Time:1.29s\n",
      "1919/2000 Loss: 1545.3112487792969, Time:1.31s\n",
      "1920/2000 Loss: 1536.8243103027344, Time:1.37s\n",
      "1921/2000 Loss: 1545.3970336914062, Time:1.41s\n",
      "1922/2000 Loss: 1536.8664855957031, Time:1.43s\n",
      "1923/2000 Loss: 1545.3514099121094, Time:1.35s\n",
      "1924/2000 Loss: 1536.8982543945312, Time:1.53s\n",
      "1925/2000 Loss: 1545.3914184570312, Time:1.42s\n",
      "1926/2000 Loss: 1536.8818664550781, Time:1.63s\n",
      "1927/2000 Loss: 1545.6687927246094, Time:1.33s\n",
      "1928/2000 Loss: 1536.806396484375, Time:1.31s\n",
      "1929/2000 Loss: 1545.7203979492188, Time:1.3s\n",
      "1930/2000 Loss: 1536.8927001953125, Time:1.41s\n",
      "1931/2000 Loss: 1545.5938415527344, Time:1.3s\n",
      "1932/2000 Loss: 1537.2251586914062, Time:1.31s\n",
      "1933/2000 Loss: 1545.3435974121094, Time:1.3s\n",
      "1934/2000 Loss: 1537.0917053222656, Time:1.3s\n",
      "1935/2000 Loss: 1546.0364074707031, Time:1.29s\n",
      "1936/2000 Loss: 1537.2894897460938, Time:1.31s\n",
      "1937/2000 Loss: 1545.6154174804688, Time:1.29s\n",
      "1938/2000 Loss: 1537.1705627441406, Time:1.37s\n",
      "1939/2000 Loss: 1545.4611511230469, Time:1.37s\n",
      "1940/2000 Loss: 1537.0465087890625, Time:1.3s\n",
      "1941/2000 Loss: 1545.5911254882812, Time:1.3s\n",
      "1942/2000 Loss: 1537.0383911132812, Time:1.29s\n",
      "1943/2000 Loss: 1546.1777038574219, Time:1.29s\n",
      "1944/2000 Loss: 1537.3298034667969, Time:1.3s\n",
      "1945/2000 Loss: 1545.2155151367188, Time:1.3s\n",
      "1946/2000 Loss: 1537.0349426269531, Time:1.31s\n",
      "1947/2000 Loss: 1545.5548400878906, Time:1.31s\n",
      "1948/2000 Loss: 1537.0580444335938, Time:1.3s\n",
      "1949/2000 Loss: 1546.1122436523438, Time:1.3s\n",
      "1950/2000 Loss: 1537.27978515625, Time:1.3s\n",
      "1951/2000 Loss: 1545.7173767089844, Time:1.29s\n",
      "1952/2000 Loss: 1537.2758178710938, Time:1.46s\n",
      "1953/2000 Loss: 1545.4535522460938, Time:1.29s\n",
      "1954/2000 Loss: 1537.1059875488281, Time:1.32s\n",
      "1955/2000 Loss: 1546.4236450195312, Time:1.3s\n",
      "1956/2000 Loss: 1537.2670593261719, Time:1.29s\n",
      "1957/2000 Loss: 1545.62939453125, Time:1.29s\n",
      "1958/2000 Loss: 1537.1741638183594, Time:1.3s\n",
      "1959/2000 Loss: 1545.9947204589844, Time:1.3s\n",
      "1960/2000 Loss: 1537.2517395019531, Time:1.3s\n",
      "1961/2000 Loss: 1545.984130859375, Time:1.3s\n",
      "1962/2000 Loss: 1537.2837524414062, Time:1.29s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1963/2000 Loss: 1545.7943115234375, Time:1.29s\n",
      "1964/2000 Loss: 1537.0230407714844, Time:1.49s\n",
      "1965/2000 Loss: 1545.977783203125, Time:1.37s\n",
      "1966/2000 Loss: 1537.124267578125, Time:1.3s\n",
      "1967/2000 Loss: 1545.9413757324219, Time:1.36s\n",
      "1968/2000 Loss: 1537.3738403320312, Time:1.29s\n",
      "1969/2000 Loss: 1545.6018371582031, Time:1.3s\n",
      "1970/2000 Loss: 1537.1973571777344, Time:1.3s\n",
      "1971/2000 Loss: 1546.0220947265625, Time:1.31s\n",
      "1972/2000 Loss: 1537.2389221191406, Time:1.29s\n",
      "1973/2000 Loss: 1546.1632995605469, Time:1.49s\n",
      "1974/2000 Loss: 1537.3016052246094, Time:1.29s\n",
      "1975/2000 Loss: 1545.8391723632812, Time:1.3s\n",
      "1976/2000 Loss: 1537.1507568359375, Time:1.29s\n",
      "1977/2000 Loss: 1546.005859375, Time:1.3s\n",
      "1978/2000 Loss: 1537.3264770507812, Time:1.31s\n",
      "1979/2000 Loss: 1545.7381591796875, Time:1.3s\n",
      "1980/2000 Loss: 1537.0865783691406, Time:1.31s\n",
      "1981/2000 Loss: 1546.0986633300781, Time:1.29s\n",
      "1982/2000 Loss: 1537.2257080078125, Time:1.31s\n",
      "1983/2000 Loss: 1546.1279296875, Time:1.3s\n",
      "1984/2000 Loss: 1537.1750183105469, Time:1.36s\n",
      "1985/2000 Loss: 1545.7345275878906, Time:1.39s\n",
      "1986/2000 Loss: 1537.2738342285156, Time:1.31s\n",
      "1987/2000 Loss: 1545.9075622558594, Time:1.3s\n",
      "1988/2000 Loss: 1537.3229675292969, Time:1.29s\n",
      "1989/2000 Loss: 1545.7002563476562, Time:1.31s\n",
      "1990/2000 Loss: 1537.2813110351562, Time:1.3s\n",
      "1991/2000 Loss: 1546.06005859375, Time:1.3s\n",
      "1992/2000 Loss: 1537.1834411621094, Time:1.3s\n",
      "1993/2000 Loss: 1546.0793151855469, Time:1.4s\n",
      "1994/2000 Loss: 1537.1268005371094, Time:1.41s\n",
      "1995/2000 Loss: 1546.0231628417969, Time:1.29s\n",
      "1996/2000 Loss: 1537.1229248046875, Time:1.3s\n",
      "1997/2000 Loss: 1545.6086120605469, Time:1.29s\n",
      "1998/2000 Loss: 1537.1878967285156, Time:1.29s\n",
      "1999/2000 Loss: 1545.6512756347656, Time:1.43s\n",
      "2000/2000 Loss: 1537.1257934570312, Time:1.34s\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "train_err_list = []\n",
    "test_err_list = []\n",
    "\n",
    "model.train()\n",
    "TIME=time.time()\n",
    "reduce=1000000\n",
    "\n",
    "for epoch in range(1,EPOCHS+1):\n",
    "    #model.train()\n",
    "    loss = train(reduce)\n",
    "    print(\"{}/{} Loss: {}, Time:{}s\".format(epoch,EPOCHS,loss,np.around(time.time()-TIME,2)))\n",
    "    TIME=time.time()\n",
    "    #model.eval()\n",
    "    train_err = evaluate(train_loader,reduce)\n",
    "    test_err = evaluate(test_loader, reduce)\n",
    "    \n",
    "    writer.add_scalar(\"loss\",loss,epoch)\n",
    "    writer.add_scalar(\"train_err\",train_err,epoch)\n",
    "    writer.add_scalar(\"test_err\",test_err,epoch)\n",
    "    \n",
    "    losses.append(loss)\n",
    "    train_err_list.append(train_err)\n",
    "    test_err_list.append(test_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7f9a7af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1,2)\n",
    "axs[0].plot(range(1,EPOCHS+1), losses, label=\"loss\")\n",
    "axs[1].plot(range(1,EPOCHS+1), train_err_list, label=\"train_err\")\n",
    "axs[1].plot(range(1,EPOCHS+1), test_err_list, label=\"test_err\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3655224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be13f150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7b52f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
